# Fine-Tuning Tutorial: Alpaca

While Levanter's main focus is pretraining, it can also be used for fine-tuning.
As an example, we'll show how to reproduce [Stanford Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html,
using either Llama 1 or [Llama 2](https://ai.meta.com/llama/) model and [Levanter](https://github.com/stanford-crfm/levanter).
The script we develop will obviously be tuned for Alpaca, using its dataset
and prompts, but it should work for any single-turn instruction-following task.
We'll use a TPU V3-32 VM, though this same tutorial should work on an A100 box as well.

This tutorial is meant to cover "full finetuning", where you start with a pretrained model and finetune it on a downstream
task, rather than something like LoRA (though see our [Alpaca LoRA tutorial](./LoRA.md) for that). It also
focuses on the case where you want to do something fancy with the dataset, like interpolate prompts into the input.

We'll use the [original Alpaca script](https://github.com/tatsu-lab/stanford_alpaca/blob/main/train.py) as a reference,
and highlight the differences between it and the Levanter version.

## Overview of Alpaca

[Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) is a lightweight fine tune of Llama 1 on a
[dataset of 52000 input/output pairs](https://huggingface.co/datasets/tatsu-lab/alpaca), which
were generated by taking [a seed set from self-instruct](https://github.com/yizhongw/self-instruct) and asking text-davinci-003 to generate
more examples.

![Schematic diagram of how the Alpaca model was created.](https://crfm.stanford.edu/static/img/posts/2023-03-13-alpaca/alpaca_main.jpg)

### The Foundation Model

Llama 1 is a 7B parameter causal language model trained on 1T tokens from various mostly English sources. It's described
in [the Llama paper](https://arxiv.org/abs/2302.13971).

### The Data

More precisely, the dataset is composed of triples of (instruction, input, output), where the instruction is a prompt
describing the task. A bit less than 40% of the examples have inputs, and the rest are just the instruction and output.

Here are some example inputs, instructions, and outputs:

| Instruction                                                     | Input                                             | Output                                                     |
|-----------------------------------------------------------------|---------------------------------------------------|------------------------------------------------------------|
| Translate the following phrase into French.                     | I love you.                                       | Je t'aime.                                                 |
| Compute the area of a rectangle with length 10cm and width 5cm. |                                                   | The area of the rectangle is 50 cm2.                       |
| Classify the following statement as true or false.              | The Supreme Court is the highest court in the US. | True                                                       |
| Name two types of desert biomes.                                |                                                   | Two types of desert biomes are xeric and subpolar deserts. |

Not all of the examples make a lot of sense, some are just plain wrong, and some are weird. (The dataset was
generated by an LLM after all.) But it's a good example of the kind of data you might want to fine tune on.

### Preprocessing

Because Llama 1 is a causal language model, we need to do some preprocessing to turn the pairs/triples into
a single sequence. The original Alpaca script does this by interpolating a prompt, depending on whether there's
an input or not.

For example, the first example above would be turned into:

```
Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
Translate the following phrase into French.

### Input:
I love you.

### Response:
Je t'aime.
```

While the second would be:

```
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Compute the area of a rectangle with length 10cm and width 5cm.

### Response:
The area of the rectangle is 50 cm2.
```

From there the Alpaca script *masks out the loss* for all tokens before the start of the output. This gets
the model to learn to mimic outputs conditioned on inputs, rather than spending time learning to generate inputs and outputs.

## Approach

Levanter's existing main entry points are designed for "pure" causal language modeling, where you have a single sequence
and don't want to mask out any tokens. So we'll instead write a custom script that does the following:

* Preprocesses the dataset into a single sequence, interpolating prompts as we go. We'll also construct a loss_mask and any padding.
* Loads the model and resizes the vocabulary to match the tokenizer.
* Runs the training loop.
* Export the final model to Hugging Face.

## TL;DR

If you just want to run something, here's the command line for Llama 1:

```bash
python levanter/examples/alpaca.py \
--config_path levanter/examples/alpaca.yaml \
--trainer.checkpointer.base_path somewhere \
--hf_save_path somewhere_else
```

You'll likely want to set `--trainer.id` to something unique if you're using preemptible VMs so you can easily restart
the job if it gets preempted.

If you want to use Llama 2, you'll need to [request access to the model](https://huggingface.co/meta-llama/Llama-2-7b-hf) and request access to the model.
Once you have access, you can run:

```bash
export HUGGING_FACE_HUB_TOKEN=${YOUR TOKEN HERE}
python levanter/examples/alpaca.py \
--config_path levanter/examples/alpaca-llama2.yaml \
--trainer.checkpointer.base_path somewhere \
--hf_save_path somewhere_else
```

### Original Alpaca Config

In case you want to change the config, here's our config for Llama 1:

TODO: update for GPU

```yaml
# cf https://github.com/tatsu-lab/stanford_alpaca#fine-tuning
model_name_or_path: huggyllama/llama-7b
trainer:
  mp: p=f32,c=bfloat16
  wandb:
    project: "levanter-alpaca"
  num_train_steps: 1218  # 128 * 1218 = 155904, which is almost but not quite 3 epochs, which is what alpaca did
  train_batch_size: 128
  per_device_parallelism: 4
  # if using model parallelism, this is useful:
  tensor_parallel_axes: ["mlp", "heads"]
optimizer:
  learning_rate: 2e-5
  weight_decay: 0.0
```

And for Llama 2:

```yaml
# cf https://github.com/tatsu-lab/stanford_alpaca#fine-tuning
model_name_or_path: meta-llama/Llama-2-7b-hf
trainer:
  mp: p=f32,c=bfloat16
  wandb:
    project: "levanter-alpaca"
    tags: ["llama2"]
  num_train_steps: 1218  # 128 * 1218 = 155904, which is almost but not quite 3 epochs, which is what alpaca did
  train_batch_size: 128
  per_device_parallelism: 2  # 2 is the max for llama-2-7b right now on a v3-32 right now

  # if using model parallelism, this is useful:
  tensor_parallel_axes: ["mlp", "heads"]
optimizer:
  learning_rate: 2e-5
  weight_decay: 0.0
```

### Customizing the Config

If you have your own dataset, you'll want to change the `data` field in the config to point to your dataset.
You'll also want to change the `model_name_or_path` field to point to the model you want to use.
Currently, Levanter supports GPT-2, Llama, MPT, and Backpack checkpoints.

# Code Walkthrough

## Preparing the Dataset

The first step is to get the dataset. We'll use the [Hugging Face Dataset version](https://huggingface.co/datasets/tatsu-lab/alpaca)
to do this. (You can also download it directly from the [dataset page](https://huggingface.co/datasets/tatsu-lab/alpaca), but
Levanter's integration with Hugging Face datasets makes it a bit easier to use.)

```python
def _get_data_source(path_or_id):
    """The original alpaca.py used a json file, but it's since been moved to the HF dataset hub. You can use any
    dataset that's compatible with the structure of the alpaca dataset."""
    if fsspec_utils.exists(path_or_id):
        return JsonDataset([path_or_id])
    else:
        return levanter.data.dataset_from_hf(path_or_id, split="train")
```

Preprocessing in Levanter typically comes in two phases:
* creating the on-disk cache,
* transforming examples from the cache into the examples that the model expects.

Here's the first phase, where we create the cache. We basically want to interpolate the prompt with the input
and instructions, and then tokenize the result. We also want to keep track of the length of the input, so we
can mask out the loss appropriately.

```python
def mk_dataset(data_path_or_id: str, cache_dir: str, tokenizer):
    # wrap an HF dataset with Levanter's native dataset class for fancier preprocessing.
    # Levanter's native dataset class supports streaming, deteriministic, distributed preprocessing out of the box,
    # which is a bit overkill for this dataset, but it's a good example of how to use it.
    dataset = _get_data_source(data_path_or_id)

    prompt_input, prompt_no_input = PROMPT_DICT["prompt_input"], PROMPT_DICT["prompt_no_input"]

    def preprocess(batch):
        sources = [
            prompt_input.format_map(example) if example.get("input", "") != "" else prompt_no_input.format_map(example)
            for example in batch
        ]
        targets = [f"{example['output']}{tokenizer.eos_token}" for example in batch]
        # TODO: this seems pretty wasteful since you end up tokenizing twice, but it's how the original code does it.
        examples = [s + t for s, t in zip(sources, targets)]
        sources_tokenized = tokenizer(sources, return_tensors="np", padding=False, truncation=True)
        examples_tokenized = tokenizer(examples, return_tensors="np", padding=False, truncation=True)

        source_lens = [len(s) for s in sources_tokenized["input_ids"]]

        return {
            "input_ids": examples_tokenized["input_ids"],
            "source_lens": source_lens,
        }

    dataset = dataset.map_batches(preprocess, batch_size=128, num_cpus=num_cpus_used_by_tokenizer(tokenizer))
    dataset = dataset.build_cache(cache_dir, await_finished=True)

    # SupervisedDataset does last minute padding and masking
    dataset = SupervisedDataset(dataset, tokenizer)

    return dataset
```

In the second, we create [levanter.models.lm.LmExample][] objects from the cache. These are the inputs to the model.
`LmExample`s look like this:

```python
class LmExample(eqx.Module):
    tokens: hax.NamedArray
    attn_mask: AttnMask
    loss_mask: hax.NamedArray
```

So we need to populate these fields. We'll do that with one last preprocessing step:

TODO: this next bit is aspirational.

```python
# JAX has to recompile for each different shape of input, so we pad to a multiple of 128
BATCH_MULTIPLE = 128

def postprocess(batch):
    # first we need to pad the input_ids to the same length
    batch = tokenizer.pad(batch,
                          return_tensors="np",
                          padding="longest",
                          truncation=True,
                          pad_to_multiple_of=BATCH_MULTIPLE,
                          return_attention_mask=False)

    input_ids = hax.named(batch["input_ids"], ("batch", "position"))
    Batch, Pos = input_ids.resolve_axis("batch", "position")

    # mask out padding and anything before the start of the target
    loss_mask = hax.arange(Pos) >= hax.array(batch["source_lens"], Batch)
    # don't compute loss when next token is padding
    loss_mask = loss_mask & (hax.roll(input_ids, -1, "position") != tokenizer.pad_token_id)

    # causal mask needs both the position and the key position
    KeyPos = Pos.alias("key_position")
    attn_mask = CausalMask(Pos, KeyPos)

    return LmExample(input_ids, attn_mask, loss_mask)

dataset = dataset.map_batches(postprocess, batch_size=config.trainer.train_batch_size, num_cpus=1)
```

## The rest

The rest is pretty boilerplate-y: setting up the model, optimizer, and trainer, and then running the training loop.
We'll skip over that in this tutorial, but you can see the full script [here](https://github.com/stanford-crfm/levanter/blob/main/examples/alpaca.py)


## Quick TPU Guide

### Setting up a TPU VM

First, we'll spin up a TPU VM using the [Getting Started with TPUs](./Getting-Started-TPU-VM.md) guide.
If you haven't gone through that guide before, you should do so now. If you have, you can just run, e.g.:

```bash
bash infra/spin-up-vm.sh llama-32 -z us-east1-d -t v3-32 --preemptible
```

### Original Alpaca Config

```yaml
# cf https://github.com/tatsu-lab/stanford_alpaca#fine-tuning
model_name_or_path: huggyllama/llama-7b
trainer:
  mp: p=f32,c=bfloat16
  wandb:
    project: "levanter-alpaca"
  num_train_steps: 1218  # 128 * 1218 = 155904, which is almost but not quite 3 epochs, which is what alpaca did
  train_batch_size: 128
  per_device_parallelism: 4
  # if using model parallelism, this is useful:
  tensor_parallel_axes: ["mlp", "heads"]
optimizer:
  learning_rate: 2e-5
  weight_decay: 0.0
```

This config uses mixed fp32/bf16 precision and sets the number of training steps to be roughly 3 epochs. It sets up the optimizer
to use a learning rate of 2e-5 and no weight decay. `trainer.per_device_parallelism` is roughly equivalent to HF's
`per_device_train_batch_size`. If you want to use model parallelism, you can set `trainer.model_axis_size` to something
like 2. (This will split the model across two devices. This might be useful if you're using a v3-64 or something similar and
want to maintain the same batch size.)

### Llama 2 Config

The [Llama 2 config](https://github.com/stanford-crfm/levanter/blob/main/examples/alpaca-llama2.yaml) is identical,
except for the HF model name and `per_device_parallelism`. The reason it's different
is that Llama 2's width is 4096 tokens instead, and it pushes us over the line for the number of examples we can fit
on a single TPU.

If you haven't already, go to [Llama 2's Hugging Face page](https://huggingface.co/meta-llama/Llama-2-7b-hf) and request access to the model.

Once you have access, go to [Hugging Face's Tokens page](https://huggingface.co/settings/tokens) to get an API token. You'll need to provide this
to the TPU VM as an environment variable. (We'll show you how to do this later.)


### Changing the config

If you make changes to the config, you'll need to get the config file to all the workers. The best way to do this
is to copy it to Google Cloud Storage so that it persists when the machine is preempted. You can do this with:

```bash
gsutil cp levanter/examples/alpaca.yaml gs://<somewhere>/train-alpaca.yaml
```

If using Llama 2:

```bash
gsutil cp levanter/examples/alpaca-llama2.yaml gs://<somewhere>/train-alpaca.yaml
```

And then using `--config_path gs://<somewhere>/alpaca.yaml` instead of `--config_path levanter/examples/train-alpaca.yaml`
in the command line below.

## Launching the job

Now we can launch the job. We need just a tiny bit of ceremony to get the Hugging Face and WANDB API tokens in the environment:
(If you're using Llama 1, you don't need the `HUGGING_FACE_HUB_TOKEN` line.)

```bash
gcloud compute tpus tpu-vm ssh llama-32 --zone us-east1-d --worker=all \
--command="WANDB_API_KEY=${YOUR TOKEN HERE} \
HUGGING_FACE_HUB_TOKEN=${YOUR TOKEN HERE} \
bash levanter/infra/run.sh python \
levanter/examples/alpaca.py \
--config_path levanter/examples/alpaca.yaml \
--trainer.checkpointer.base_path gs://<somewhere> \
--hf_save_path gs://<somewhere> \
```

If you're using preemptible or TRC TPUs, you'll want to add `--trainer.id <some id>` to the command line,
and probably use the [babysitting script](./Getting-Started-TPU-VM.md#babysitting-script) to automatically restart the
vm and job if it gets preempted. That would look like this:

```bash
infra/babysit-tpu-vm.sh llama-32 -z us-east1-d -t v3-32 --preemptible -- \
WANDB_API_KEY=${YOUR TOKEN HERE} \
HUGGING_FACE_HUB_TOKEN=${YOUR TOKEN HERE} \
bash levanter/infra/run.sh python \
levanter/examples/alpaca.py \
--config_path levanter/examples/alpaca-llama2.yaml \
--trainer.checkpointer.base_path gs://<somewhere> \
--hf_save_path gs://<somewhere> \
```


## Waiting

At some point it will spit out a Wandb link. You can click on that to see the training progress. There's
not a ton to see here (yet), but you can see the training loss go down over time.

Llama 1 should take about ~3.5 hours on a v3-32 (which is more or less in line with A100 times). Unfortunately, LLama 2
is much slower because of the much longer max sequence length of 4096 and the resulting requirement to do gradient
accumulation to fit on the TPU. It should take about ~9 hours on a v3-32.
