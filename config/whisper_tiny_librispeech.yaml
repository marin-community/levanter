data:
  id: WillHeld/librispeech_parquet
  cache_dir: "gs://public_data_lev/processed/librispeech"
  train_split: "train.360"
  text_key: "text"
  # The Whisper Tokenizer is way too large for Librispeech
  tokenizer: "facebook/wav2vec2-base-960h"
model:
  type: whisper
  vocab_size: 32
  bos_token_id: 1
  eos_token_id: 2
  pad_token_id: 0
trainer:
  tracker:
    - type: wandb
      project: "levanter"
      tags: [ "librispeech", "whisper"]

  mp: p=f32,c=bf16
  model_axis_size: 1
  per_device_parallelism: -1

  train_batch_size: 128
  num_train_steps: 16000
optimizer:
  learning_rate: 3E-3
  weight_decay: 0.1
  warmup: 0.01
