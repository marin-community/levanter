# Config adapted from sft_tootsie_mixture.yaml for use with train_lm.py
# Trains only on the tulu dataset using the chat format.

data:
  # Using LMMixtureDatasetConfig structure like gpt2_small_fast_mix_chat.yaml
  configs:
    tulu:
      id: allenai/tulu-3-sft-mixture
      format:
        type: "chat"
  train_weights:
    tulu: 1.0 # Weight for the single dataset
  tokenizer: stanford-crfm/marin-olmo2-tokenizer
  cache_dir: "gs://marin-us-central2/tokenized/marin-olmo2-tokenizer-/tulu-3-sft-mixture"
  shuffle: true
  # permutation_type: "feistel" # Removed due to draccus parsing error for Literal type
  # cache_dir: # Can optionally specify a top-level cache dir for the mixture if needed

model:  # Olmo2 7B model config
  type: olmo2
  seq_len: 4096
  hidden_dim: 4096
  intermediate_dim: 11008
  num_layers: 32
  num_heads: 32
  num_kv_heads: 32
  use_flash_attention: True
  flash_attention_block_size: 512
  use_bias: false
  use_layer_norm_weight: true
  initializer_range: 0.02
  layer_norm_epsilon: 1e-6
  activation_function: "silu"
  attention_bias: false
  upcast_attn: True
  rope:
    type: "default"
    theta: 500000

trainer:
  seed: 0
  tracker:
    type: wandb
    project: "marin"
    tags: ["dolma", "olmo", "llama", "tulu", "train_lm"] # Adjusted tags
  wandb:
    project: "marin"
    name: "train_lm_olmo2_tulu_lr1e-4" # Adjusted name

  mp: p=f32,c=bfloat16
  train_batch_size: 128
  num_train_steps: 5112
  steps_per_eval: 1000 # Note: No eval dataset specified, so this might not do much unless one is added
  tensor_parallel_axes: ["mlp", "heads"]
  fsdp_axis: "embed"
  batch_axis: "batch"
  checkpointer:
    base_path: "gs://marin-us-central2/checkpoints/train_lm_olmo2_tulu_lr1e-4/seed_0/" # Adjusted path

optimizer:
  learning_rate: 1e-4
  weight_decay: 0.0
  min_lr_ratio: 0.0
  lr_schedule: "linear"
  warmup: 0.03

# Initialization from the specific HF checkpoint used in sft_tootsie_mixture.yaml
initialize_from_hf: "allenai/OLMo-2-1124-7B"
use_hf_model_config: False # Use the model config defined above

# HF Saving config from sft_tootsie_mixture.yaml
hf_save_steps: 1000
hf_save_path: "gs://marin-us-central2/checkpoints/train_lm_olmo2_tulu_lr1e-4/hf/seed_0/" # Adjusted path

# Defaults or settings not applicable/present in sft_tootsie_mixture.yaml for train_lm:
# z_loss_weight: 0.0
# epoch: 0
# data_seed: None
# eval_harness: None
# eval_harness_steps: 10000
# log_entropy: False
# reinit_tokens: Not supported by train_lm.py
