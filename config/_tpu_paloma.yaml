data:
  cache_dir: gs://data-values/data_cache/dolma_olmo_paloma
  configs:
    dolma-algebraic-stack:
      train_urls:
        - gs://marin-us-central2/raw/dolma/v1.7/algebraic-stack-train-{0000..0015}.json.gz
    dolma-arxiv:
      train_urls:
        - gs://marin-us-central2/raw/dolma/v1.7/arxiv-{0000..0099}.json.gz
    dolma-stackexchange:
      train_urls:
        - gs://marin-us-central2/raw/dolma/v1.7/stackexchange-{0000..0025}.json.gz
    # these are just for eval
    "paloma/dolma_100_programing_languages":
      validation_urls:
        - gs://levanter-data/paloma/dolma_100_programing_languages/val/val*.jsonl.gz
  train_weights:
    # sampling proportion comes from https://huggingface.co/datasets/allenai/dolma
    dolma-algebraic-stack: 1.0 # 12.6 * 1.0
    dolma-arxiv: 1.0 # 28.0 * 1.0
    dolma-stackexchange: 1.0 # 1.0
    paloma/dolma_100_programing_languages: 0.0
  #id: dlwh/wikitext_103_detokenized
  #id: Tristan/RedPajama-Data-V2-sample-100B-filtered-shuffled-tokenized-with-token-counts
model:
  type: gpt2
  hidden_dim: 768
  num_heads: 12
  num_layers: 12
  seq_len: 128
  gradient_checkpointing: true
  layer_norm_epsilon: 1e-2
  qk_norm: true
trainer:
  mp: p=f32,c=f32
  model_axis_size: 1
  per_device_parallelism: 32
  per_device_eval_parallelism: 32

  profiler_tpu_trace_mode: "TRACE_COMPUTE_AND_SYNC"
  profiler: true
  profiler_start_step: 4
  profiler_num_steps: 2
  backward_profiler: true
  backward_profiler_start_step: 6
  backward_profiler_num_steps: 2
  log_dir: "jax_profile_traces"

  train_batch_size: 1024
  num_train_steps: 10
  max_eval_batches: 1

  ray:
    auto_start_cluster: false

  tracker:
    - type: wandb
      project: "levanter"
      name: "tpu_test_profile"
      save_xla_dumps: true

  checkpointer:
    keep:
      - every: 1
    save_interval: 5m
    base_path: "gs://data-values/checkpoints/test/"


  load_debug_weights: false

  #id: llama_1k_yolo
  #load_checkpoint_path: "/scr-ssd/sampark/levanter/checkpoints/gpt2_1k_yolo_ln_before"

  metagrad_checkpoint_frequency: 20
  metagrad_segment_size: 6

  # Enable logging of gradients, parameters, and Adam optimizer state (mu and nu)
  watch:
    watch_targets: ["grads", "params"]
    include_norms: true
    include_per_parameter_norms: true
    interval: 10

  jax_compilation_cache_dir: "jax_compilation_cache"

out_dir: out_dir_test
optimizer:
  lr_schedule: linear
  learning_rate: 1E-4
  weight_decay: 0.1
  warmup: 0.1