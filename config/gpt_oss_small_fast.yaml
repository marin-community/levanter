# GPT-OSS SFT Configuration
  # Modified from the original to focus on SFT training

  data:
    id: kothasuhas/dclm_10B_tokens
    cache_dir: "gs://marin-us-central2/tokenized/dclm_10B_tokens_gpt_oss-cbbde7"
    tokenizer: "openai/gpt-oss-20b"

  # Same model configuration
  model:
    type: gpt_oss
    seq_len: 1024                    # Memory-efficient sequence length
    hidden_dim: 768
    intermediate_dim: 3072
    num_layers: 12
    num_heads: 12
    num_kv_heads: 4
    num_local_experts: 4
    num_experts_per_tok: 2
    router_aux_loss_coef: 0.01
    output_router_logits: false
    layer_types: ["full_attention", "sliding_attention"]
    sliding_window: 256
    use_bias: false
    layer_norm_epsilon: 1e-5
    initializer_range: 0.02
    gradient_checkpointing: true     # CRITICAL: Enable gradient checkpointing to save memory
    flash_attention_block_size: 128

  # SFT-specific training configuration
  trainer:
    tracker:
      - type: wandb
        project: "gpt-oss"
        name: "gpt-oss-small-fast"
        tags: ["gpt-oss", "sft", "tulu-3"]

    mp: p=f32,c=bfloat16
    model_axis_size: 1               # TPU parallelism setting
    per_device_parallelism: -1       # Auto-calculate based on batch size
    train_batch_size: 128            # TPU v4-128 minimum batch size (128 devices)
    num_train_steps: 3000            # More steps for SFT
    steps_per_eval: 500              # Less frequent eval

    tensor_parallel_axes: ["heads"]
    fsdp_axis: "embed"
    batch_axis: "batch"

    checkpointer:
      base_path: "gs://marin-us-central2/checkpoints/gpt_oss_dclm_10B_tokens/"
      save_interval: 30m

  # SFT-specific optimizer (lower learning rate)
  optimizer:
    learning_rate: 5e-6              # Much lower LR for SFT
    weight_decay: 0.0                # No weight decay for SFT
    min_lr_ratio: 0.0
    lr_schedule: "linear"            # Linear decay
    warmup: 0.03                     # 3% warmup

  # HF model initialization (load pretrained base model)
  initialize_from_hf: false  # Start from base model
  use_hf_model_config: false               # Use our model config above

  hf_save_steps: 500
  hf_save_path: "gs://marin-us-central2/checkpoints/gpt_oss_dclm_10B_tokens/hf/"