data: !include data/dolma_olmo_paloma.yaml
model:
  type: olmo
initialize_from_hf: "allenai/OLMo-1.7-7B-hf@step0-tokens0B"
use_hf_model_config: true

trainer:
  ray:
    auto_start_cluster: false
  seed: 42
  checkpointer:
    keep:
      - every: 100
        until: 200
      - every: 1000
        until: 10000
      - every: 10000
        until: 40000
  tracker:
    type: wandb
    project: "trace-train"
    tags: ["pile", "olmo", "web_comparison"]
  mp: p=f32,c=bfloat16
  model_axis_size: 1
  per_device_eval_parallelism: -1
  max_eval_batches: 2
  train_batch_size: 1024
  num_train_steps: 50000
  steps_per_eval: 10
  tensor_parallel_axes: ["mlp", "heads"]
  fsdp_axis: "embed"
  batch_axis: "batch"

optimizer:
# lr from olmo (they decay up to 21 billion tokens from 3E-4)
  learning_rate: 3E-4
  weight_decay: 0.1
  min_lr_ratio: 0.1

hf_save_steps: 10000
hf_save_path: "gs://levanter-checkpoints/olmo_trace_hf/"  # TODO
data_seed: 0
