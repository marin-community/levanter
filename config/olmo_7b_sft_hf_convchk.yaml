model:  # 7B class model
  type: llama
  seq_len: 2048
  # hidden_dim: 4096
  # intermediate_dim: 11008
  # num_layers: 32
  # num_heads: 32
  # num_kv_heads: 32
  # use_flash_attention: True
  # use_bias: false
  use_layer_norm_weight: false
  #flash_attention_block_size: 1024
# OLMO SFT config below
