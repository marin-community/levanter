hf_checkpoint: "meta-llama/Meta-Llama-3.1-8B"
tokenizer: "meta-llama/Llama-3.2-1B"
trainer:
  ray:
    auto_start_cluster: false
  mp: p=f32,c=bfloat16
  tensor_parallel_axes: ["mlp", "heads", "kv_head", "vocab"]
  model_axis_size: 4
