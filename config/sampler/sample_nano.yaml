hf_checkpoint: "meta-llama/Llama-3.2-1B"
tokenizer: "meta-llama/Llama-3.2-1B"
trainer:
  ray:
    auto_start_cluster: false
  mp: p=bfloat16,c=bfloat16
  tensor_parallel_axes: ["mlp", "heads", "kv_head", "vocab"]
  model_axis_size: 1

service:
  max_pages: 256
  max_seqs: 16
  page_size: 8
  max_pages_per_seq: 16
  max_queued_tokens: 16
  max_seqs_in_prefill: 8
