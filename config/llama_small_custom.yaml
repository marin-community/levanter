data:
  id: roneneldan/TinyStories
model:
  type: llama
  hidden_dim: 768
  intermediate_dim: 3072
  num_heads: 12
  num_kv_heads: 12
  num_layers: 12
  seq_len: 128
  gradient_checkpointing: true
trainer:
  mp: p=f32,c=f32
  model_axis_size: 1
  per_device_parallelism: 64
  per_device_eval_parallelism: 64

  train_batch_size: 1024
  num_train_steps: 5
  max_eval_batches: 1

  ray:
    auto_start_cluster: false

  tracker:
    - type: wandb
      project: "levanter"

  checkpointer:
    keep:
      - every: 1
    save_interval: 5m
out_dir: out_dir_test
optimizer:
  lr_schedule: linear
  learning_rate: 1E-3
  weight_decay: 0.1
  warmup: 0.1