# Model configuration
model:
  type: llama
  seq_len: 4096
  hidden_dim: 4096
  intermediate_dim: 14336
  num_layers: 32
  num_heads: 32
  num_kv_heads: 8
  use_flash_attention: true
  flash_attention_block_size: 512
  use_bias: false
  use_layer_norm_weight: false
  initializer_range: 0.02
