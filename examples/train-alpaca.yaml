# cf https://github.com/tatsu-lab/stanford_alpaca#fine-tuning
#model_name_or_path: decapoda-research/llama-7b-hf
model_name_or_path: meta-llama/Llama-2-7b-hf
trainer:
  wandb:
    project: "levanter-alpaca"
  num_train_steps: 1218  # 128 * 1218 = 155904, which is almost but not quite 3 epochs, which is what alpaca did
  train_batch_size: 128
optimizer:
  learning_rate: 2e-5
  weight_decay: 0.0
