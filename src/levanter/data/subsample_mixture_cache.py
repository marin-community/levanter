import dataclasses
import logging
import os
from dataclasses import dataclass
from typing import Dict, Mapping

import fsspec
import humanfriendly
import numpy as np
from transformers import AutoTokenizer
from tqdm_loggable.auto import tqdm

from levanter.data.text import (
    LMMixtureDatasetConfig,
    LmDatasetSourceConfigBase,
    HfDatasetSourceConfig,
    UrlDatasetSourceConfig,
)
from levanter.store import SerialCacheWriter, TreeCache

logger = logging.getLogger(__name__)


def _short_desc_from_lm_config(config: LmDatasetSourceConfigBase) -> str:
    """Return a short markdown description of the dataset source."""
    if isinstance(config, HfDatasetSourceConfig):
        url = f"[{config.id}](https://huggingface.co/datasets/{config.id})"
        if config.name:
            url += f" (name: {config.name})"
        return url
    if isinstance(config, UrlDatasetSourceConfig):
        out = ""
        if config.train_urls:
            out += "Train Urls:\n" + "".join(f"- {u}\n" for u in config.train_urls)
        if config.validation_urls:
            out += "Validation Urls:\n" + "".join(f"- {u}\n" for u in config.validation_urls)
        return out or "{missing urls}"
    return ""


@dataclass
class SubsampleMixtureConfig:
    """Configuration for subsampling a mixture cache."""

    mixture_config: LMMixtureDatasetConfig
    num_tokens: int
    cache_path: str
    tokenizer: str = "stanford-crfm/marin-tokenizer"
    seed: int = 42


def _patch_source_config(
    input_config: LmDatasetSourceConfigBase, output_path: str, extra_tags: list[str]
) -> LmDatasetSourceConfigBase:
    return dataclasses.replace(
        input_config,
        cache_dir=output_path,
        tags=(input_config.tags or []) + extra_tags,
    )


def _create_readme(
    output_path: str, input_config: LmDatasetSourceConfigBase, num_tokens: int, tokenizer_spec: str, seed: int
):
    readme_path = os.path.join(output_path, "README.md")
    with fsspec.open(readme_path, "w") as f:
        f.write("# Marin/Levanter Subsampled Pretokenized Dataset\n\n")
        f.write("## Dataset\n\n")
        f.write(_short_desc_from_lm_config(input_config))
        f.write("\n\n## Factsheet\n\n")
        f.write(f"* Original cache: {input_config.cache_dir}\n")
        f.write(f"* Tokenizer: [{tokenizer_spec}](https://huggingface.co/{tokenizer_spec})\n")
        f.write(f"* Seed {seed}\n")
        f.write(f"* Number of tokens: {humanfriendly.format_number(num_tokens)}\n")
        f.write("\n\n(This readme is automatically generated by Marin.)\n")


def subsample_mixture_caches(cfg: SubsampleMixtureConfig) -> Mapping[str, LmDatasetSourceConfigBase]:
    """Subsample each component of a mixture dataset according to its weight."""

    tokenizer = AutoTokenizer.from_pretrained(cfg.tokenizer)
    rng = np.random.default_rng(cfg.seed)

    weights = cfg.mixture_config.train_weights
    if not isinstance(weights, Mapping):
        raise ValueError("subsampling only supported for constant weights")
    total_weight = sum(weights.values())

    out_configs: Dict[str, LmDatasetSourceConfigBase] = {}

    for name, source_cfg in cfg.mixture_config.configs.items():
        weight = weights.get(name, 0.0)
        if weight <= 0:
            continue

        target_tokens = int(cfg.num_tokens * weight / total_weight)
        cache = source_cfg.load_cache("train", tokenizer)
        store = cache.store.tree["input_ids"]
        num_docs = store.num_rows
        offsets = store.offsets[: num_docs + 1].read().result()
        offsets[0] = 0
        lengths = np.diff(offsets)

        permuted = rng.permutation(num_docs)
        selected = []
        loaded_tokens = 0
        pbar = tqdm(total=target_tokens, desc=f"Sampling {name}", unit="token")
        for idx in permuted:
            selected.append(idx)
            loaded_tokens += int(lengths[idx])
            pbar.update(int(lengths[idx]))
            if loaded_tokens >= target_tokens:
                break
        pbar.close()

        if loaded_tokens < target_tokens:
            raise ValueError(
                f"Cache {name} does not have enough tokens: {loaded_tokens} < {target_tokens}"
            )

        selected.sort()
        exemplar = cache[0]
        out_dir = os.path.join(cfg.cache_path, name, "train")
        os.makedirs(out_dir, exist_ok=True)

        with SerialCacheWriter(out_dir, exemplar) as writer:
            BS = 4096
            for start in range(0, len(selected), BS):
                batch_indices = selected[start : start + BS]
                batch = cache.get_batch_sync(batch_indices)
                writer.write_batch(batch)

        out_cache = TreeCache.load(out_dir, exemplar)
        if out_cache.store.tree["input_ids"].data_size != loaded_tokens:
            raise ValueError(
                f"Cache size mismatch: {out_cache.store.tree['input_ids'].data_size} != {loaded_tokens}"
            )

        _create_readme(os.path.dirname(out_dir), source_cfg, loaded_tokens, cfg.tokenizer, cfg.seed)
        humanfriendly_tokens = (
            humanfriendly.format_size(loaded_tokens)[:-1].replace(" ", "").replace("byte", "")
        )
        out_configs[name] = _patch_source_config(
            source_cfg, os.path.dirname(out_dir), ["subsampled", f"subsampled-{humanfriendly_tokens}"]
        )

        logger.info(f"Wrote {loaded_tokens} tokens for {name} to {out_dir}")

    return out_configs
