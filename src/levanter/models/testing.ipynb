{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'levanter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlevanter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mroberta\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmy_roberta\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mroberta\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m modeling_roberta \u001b[38;5;28;01mas\u001b[39;00m hf_roberta\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'levanter'"
     ]
    }
   ],
   "source": [
    "import roberta as my_roberta\n",
    "from transformers.models.roberta import modeling_roberta as hf_roberta\n",
    "\n",
    "import torch\n",
    "import haliax as hax\n",
    "import jax\n",
    "import jax.random as jrandom\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "# hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "from time import time\n",
    "\n",
    "hf_model_str = \"FacebookAI/roberta-base\"\n",
    "\n",
    "hf_config = AutoConfig.from_pretrained(hf_model_str)\n",
    "hf_config.hidden_dropout_prob = 0\n",
    "hf_config.attention_probs_dropout_prob = 0\n",
    "# hf_config.pad_token_id = -1\n",
    "my_config = my_roberta.RobertaConfig.from_hf_config(hf_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 1725922495\n"
     ]
    }
   ],
   "source": [
    "seed = time()\n",
    "print(f\"seed: {int(seed)}\")\n",
    "key = jrandom.PRNGKey(int(seed))\n",
    "\n",
    "key_vars, key_funcs, key_run = jrandom.split(key, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EmbedAtt = my_config.EmbedAtt\n",
    "Embed = my_config.Embed\n",
    "Mlp = my_config.Mlp\n",
    "Pos = my_config.Pos\n",
    "KeyPos = my_config.KeyPos\n",
    "Heads = my_config.Heads\n",
    "\n",
    "cut_end_for_bounds = True \n",
    "\n",
    "Batch = hax.Axis(\"batch\", 2)\n",
    "Vocab = hax.Axis(\"vocab\", my_config.vocab_size)\n",
    "\n",
    "keys = jrandom.split(key_vars, 6)\n",
    "\n",
    "input_ids = hax.random.randint(keys[0], (Batch, Pos), minval = 3, maxval = my_config.vocab_size)\n",
    "if cut_end_for_bounds:\n",
    "    input_ids = input_ids[{\"position\": slice(0,-2)}]\n",
    "input_ids_torch = torch.from_numpy(np.array(input_ids.array))\n",
    "\n",
    "input_embeds = hax.random.normal(keys[1], (Batch, Pos, Embed))\n",
    "if cut_end_for_bounds:\n",
    "    input_embeds = input_embeds[{\"position\": slice(0,-2)}]\n",
    "input_embeds_torch = torch.from_numpy(np.array(input_embeds.array))\n",
    "\n",
    "# mask = hax.random.randint(keys[2], (Batch, Pos), minval = 0, maxval = 2)\n",
    "# mask = hax.ones((Batch, Pos))\n",
    "mask = hax.zeros((Batch, Pos))\n",
    "\n",
    "if cut_end_for_bounds:\n",
    "    mask = mask[{\"position\": slice(0,-2)}]\n",
    "mask_torch = torch.from_numpy(np.array(mask.array))\n",
    "\n",
    "mask_materialized = (mask == 0) * jnp.finfo(jnp.bfloat16).min\n",
    "mask_torch_materialized = hf_roberta.RobertaModel.get_extended_attention_mask(self=hf_roberta.RobertaModel(hf_config), attention_mask=mask_torch, input_shape=input_embeds_torch.shape)\n",
    "\n",
    "features = input_embeds[{\"position\": 0}]\n",
    "features_torch = torch.from_numpy(np.array(features.array))\n",
    "\n",
    "x_embed_att = input_embeds.rename({\"embed\": \"embed_att\"})\n",
    "if cut_end_for_bounds:\n",
    "    x_embed_att = x_embed_att[{\"position\": slice(0,-2)}]\n",
    "x_embed_att_torch = torch.from_numpy(np.array(x_embed_att.array))\n",
    "\n",
    "x_mlp = hax.random.normal(keys[5], (Batch, Pos, Mlp))\n",
    "if cut_end_for_bounds:\n",
    "    x_mlp = x_mlp[{\"position\": slice(0,-2)}]    \n",
    "x_mlp_torch = torch.from_numpy(np.array(x_mlp.array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(my_output, hf_output, precision=1e-4):\n",
    "    \n",
    "    assert (np.array(my_output.shape) == np.array(hf_output.shape)).all()\n",
    "    # print(my_output.shape)\n",
    "    # print(hf_output.shape)\n",
    "\n",
    "    acc = np.isclose(hf_output, np.array(my_output), rtol=precision, atol=precision).mean()\n",
    "\n",
    "    # stats = (torch.tensor(np.array(my_output)).abs().mean(), torch.tensor(np.array(hf_output)).abs().mean()) \n",
    "    stats = (torch.linalg.norm(torch.tensor(np.array(my_output))), torch.linalg.norm(torch.tensor(np.array(hf_output))))\n",
    "    \n",
    "    difference = torch.tensor(np.array(my_output)) - torch.tensor(np.array(hf_output))\n",
    "\n",
    "    diffs = difference.abs().mean()\n",
    "\n",
    "    to_print = f\"acc: {acc} \\t norms: {stats} \\t diffs: {diffs}\"\n",
    "    \n",
    "    return acc, stats, diffs, to_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dicts(my_dict, hf_dict):\n",
    "    print(my_dict.keys())\n",
    "    print(hf_dict.keys())\n",
    "\n",
    "    hf_keys_save = list(hf_dict.keys())\n",
    "\n",
    "    flag = 0\n",
    "    diff = 0\n",
    "\n",
    "    for k in my_dict.keys():\n",
    "        i = my_dict[k]\n",
    "        if k not in hf_dict:\n",
    "            print(f\"ERROR \\t {k}: key in my_dict but not hf_dict\")\n",
    "        j = hf_dict[k]\n",
    "        diff += (np.array(i) - np.array(j)).sum()\n",
    "        if check(i, j.detach())[0] < 1:\n",
    "            print(f\"ERROR \\t {k}: {check(i, j)[0]}\")\n",
    "            flag += 1\n",
    "        hf_keys_save.remove(k)\n",
    "\n",
    "    if flag == 0:\n",
    "        print(\"success1\") \n",
    "    else:\n",
    "        print(\"fail1\") \n",
    "\n",
    "    if len(hf_keys_save) == 0:\n",
    "        print(\"success2\") \n",
    "    else:\n",
    "        print(\"fail2\")\n",
    "        print(hf_keys_save)\n",
    "\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mstop\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing RobertaSelfOutput\n",
    "\n",
    "def test_RobertaSelfOutput(key):\n",
    "    k_1, k_2 = jrandom.split(key, 2)\n",
    "    my_func = my_roberta.RobertaSelfOutput.init(my_config, key=k_1)\n",
    "    state = my_func.to_state_dict()\n",
    "\n",
    "    state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "    # # print(state.keys())\n",
    "\n",
    "    hf_func = hf_roberta.RobertaSelfOutput(hf_config)\n",
    "    hf_func.load_state_dict(state, strict=True, assign=True)\n",
    "\n",
    "    my_output = my_func(x_embed_att, input_embeds, key=k_2)\n",
    "    hf_output = hf_func(x_embed_att_torch, input_embeds_torch)\n",
    "\n",
    "    return check(my_output.array, hf_output.detach())\n",
    "\n",
    "# Testing RobertaSelfAttention\n",
    "\n",
    "def test_RobertaSelfAttention(key):\n",
    "    k_1, k_2 = jrandom.split(key, 2)\n",
    "\n",
    "    my_func = my_roberta.RobertaSelfAttention.init(my_config, key=k_1)\n",
    "    state = my_func.to_state_dict()\n",
    "\n",
    "    state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "    # print(state.keys())\n",
    "\n",
    "    hf_func  = hf_roberta.RobertaSelfAttention(hf_config)\n",
    "    hf_func.load_state_dict(state, strict=True, assign=True)\n",
    "\n",
    "    my_output = my_func(input_embeds, mask_materialized, key=k_2)\n",
    "    hf_output = hf_func(input_embeds_torch, mask_torch_materialized)\n",
    "\n",
    "    return check(my_output.array, hf_output[0].detach())\n",
    "\n",
    "# Testing RobertaAttention\n",
    "\n",
    "def test_RobertaAttention(key):\n",
    "    k_1, k_2 = jrandom.split(key, 2)\n",
    "    my_func = my_roberta.RobertaAttention.init(my_config, key=k_1)\n",
    "    state = my_func.to_state_dict()\n",
    "\n",
    "    state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "    # print(state.keys())\n",
    "\n",
    "    hf_func = hf_roberta.RobertaAttention(hf_config)\n",
    "    hf_func.load_state_dict(state, strict=True, assign=True)\n",
    "\n",
    "    my_output = my_func(hidden_states=input_embeds, attention_mask=mask_materialized, key=k_2)\n",
    "    hf_output = hf_func(hidden_states=input_embeds_torch, attention_mask=mask_torch_materialized)\n",
    "\n",
    "    return check(my_output.array, hf_output[0].detach())\n",
    "\n",
    "# Testing RobertaIntermediate\n",
    "\n",
    "def test_RobertaIntermediate(key):\n",
    "    k_1, k_2 = jrandom.split(key, 2)\n",
    "    my_func = my_roberta.RobertaIntermediate.init(my_config, key=k_1)\n",
    "    state = my_func.to_state_dict()\n",
    "\n",
    "    state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "    # print(state.keys())\n",
    "\n",
    "    hf_func = hf_roberta.RobertaIntermediate(hf_config)\n",
    "    hf_func.load_state_dict(state, strict=True, assign=True)\n",
    "\n",
    "    my_output = my_func(input_embeds, key=k_2)\n",
    "    hf_output = hf_func(input_embeds_torch)\n",
    "\n",
    "    return check(my_output.array, hf_output.detach())\n",
    "\n",
    "# Testing RobertaOutput\n",
    "\n",
    "def test_RobertaOutput(key):\n",
    "    k_1, k_2 = jrandom.split(key, 2)\n",
    "\n",
    "    my_func = my_roberta.RobertaOutput.init(my_config, key=k_1)\n",
    "    state = my_func.to_state_dict()\n",
    "\n",
    "    state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "    # print(state.keys())\n",
    "\n",
    "    hf_func = hf_roberta.RobertaOutput(hf_config)\n",
    "    hf_func.load_state_dict(state, strict=True, assign=True)\n",
    "\n",
    "    my_output = my_func(x_mlp, input_embeds, key=k_2)\n",
    "    hf_output = hf_func(x_mlp_torch, input_embeds_torch)\n",
    "\n",
    "    return check(my_output.array, hf_output.detach())\n",
    "\n",
    "# Testing RobertaLayer\n",
    "\n",
    "def test_RobertaLayer(key):\n",
    "    k_1, k_2 = jrandom.split(key, 2)\n",
    "    my_func = my_roberta.RobertaLayer.init(my_config, key=k_1)\n",
    "    state = my_func.to_state_dict()\n",
    "\n",
    "    state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "    # print(state.keys())\n",
    "\n",
    "    hf_func = hf_roberta.RobertaLayer(hf_config)\n",
    "    hf_func.load_state_dict(state, strict=True, assign=True)\n",
    "\n",
    "    my_output = my_func(hidden_states=input_embeds, attention_mask=mask_materialized, key=k_2)\n",
    "    hf_output = hf_func(hidden_states=input_embeds_torch, attention_mask=mask_torch_materialized)\n",
    "\n",
    "    return check(my_output[0].array, hf_output[0].detach())\n",
    "\n",
    "# Testing RobertaEncoder\n",
    "\n",
    "def test_RobertaEncoder(key):\n",
    "    k_1, k_2 = jrandom.split(key, 2)\n",
    "    my_func = my_roberta.RobertaEncoder.init(my_config, key=k_1)\n",
    "    state = my_func.to_state_dict()\n",
    "\n",
    "    state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "    # print(state.keys())\n",
    "\n",
    "    hf_func = hf_roberta.RobertaEncoder(hf_config)\n",
    "    hf_func.load_state_dict(state, strict=True, assign=True)\n",
    "\n",
    "    my_output = my_func(hidden_states=input_embeds, attention_mask=mask_materialized, key=k_2)\n",
    "    hf_output = hf_func(hidden_states=input_embeds_torch, attention_mask=mask_torch_materialized)\n",
    "\n",
    "    return check(my_output[0].array, hf_output[0].detach())\n",
    "\n",
    "# Testing RobertaEmbedding\n",
    "\n",
    "def test_RobertaEmbedding(key, ids = True):\n",
    "    k_1, k_2 = jrandom.split(key, 2)\n",
    "    my_func = my_roberta.RobertaEmbedding.init(Vocab, my_config, key=k_1)\n",
    "    state = my_func.to_state_dict()\n",
    "\n",
    "    state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "    # print(state.keys())\n",
    "\n",
    "    hf_func = hf_roberta.RobertaEmbeddings(hf_config)\n",
    "    hf_func.load_state_dict(state, strict=True, assign=True)\n",
    "\n",
    "    if ids:\n",
    "        my_output = my_func.embed(input_ids=input_ids, key=k_2)\n",
    "        hf_output = hf_func(input_ids=input_ids_torch)\n",
    "    else:        \n",
    "        my_output = my_func.embed(input_embeds=input_embeds, key=k_2)\n",
    "        hf_output = hf_func(inputs_embeds=input_embeds_torch)\n",
    "\n",
    "    return check(my_output.array, hf_output.detach())\n",
    "\n",
    "# Testing RobertaPooler\n",
    "\n",
    "def test_RobertaPooler(key):\n",
    "    k_1, k_2 = jrandom.split(key, 2)\n",
    "    my_func = my_roberta.RobertaPooler.init(my_config, key=k_1)\n",
    "    state = my_func.to_state_dict()\n",
    "\n",
    "    state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "    # print(state.keys())\n",
    "\n",
    "    hf_func = hf_roberta.RobertaPooler(hf_config)\n",
    "    hf_func.load_state_dict(state, strict=True, assign=True)\n",
    "\n",
    "    my_output = my_func(input_embeds, key=k_2)\n",
    "    hf_output = hf_func(input_embeds_torch)\n",
    "\n",
    "    return check(my_output.array, hf_output.detach())\n",
    "\n",
    "# Testing RobertaModel\n",
    "\n",
    "def test_RobertaModel(key, ids = True, pool = True):\n",
    "    k_1, k_2 = jrandom.split(key, 2)\n",
    "    my_func = my_roberta.RobertaModel.init(Vocab, my_config, add_pooling_layer=pool, key=k_1)\n",
    "    state = my_func.to_state_dict()\n",
    "\n",
    "    state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "    # print(state.keys())\n",
    "\n",
    "    hf_func = hf_roberta.RobertaModel(hf_config, add_pooling_layer=pool)\n",
    "    hf_func.load_state_dict(state, strict=True, assign=True)\n",
    "\n",
    "    if ids:\n",
    "        my_output = my_func(input_ids = input_ids, attention_mask=mask, key=k_2)\n",
    "        hf_output = hf_func(input_ids = input_ids_torch, attention_mask=mask_torch, return_dict=False)\n",
    "    else:\n",
    "        my_output = my_func(input_embeds = input_embeds, attention_mask=mask, key=k_2)\n",
    "        hf_output = hf_func(inputs_embeds = input_embeds_torch, attention_mask=mask_torch, return_dict=False)\n",
    "\n",
    "    if pool:\n",
    "        return check(my_output[1].array, hf_output[1].detach())\n",
    "    else:\n",
    "        return check(my_output[0].array, hf_output[0].detach())\n",
    "\n",
    "# Testing RobertaLMHead\n",
    "\n",
    "def test_RobertaLMHead(key):\n",
    "    k_1, k_2 = jrandom.split(key, 2)\n",
    "    my_func = my_roberta.RobertaLMHead.init(Vocab, my_config, key=k_1)\n",
    "    state = my_func.to_state_dict()\n",
    "\n",
    "    state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "    state[\"bias\"] = torch.zeros(hf_config.vocab_size)\n",
    "\n",
    "    # print(state.keys())\n",
    "\n",
    "    hf_func = hf_roberta.RobertaLMHead(hf_config)\n",
    "    hf_func.load_state_dict(state, strict=True, assign=True)\n",
    "\n",
    "    my_output = my_func(features, key=k_2)\n",
    "    hf_output = hf_func(features_torch)\n",
    "\n",
    "    return check(my_output.array, hf_output.detach())\n",
    "\n",
    "# Testing RobertaForMaskedLM\n",
    "\n",
    "def test_RobertaForMaskedLM(key, ids = True):\n",
    "    k_1, k_2 = jrandom.split(key, 2)\n",
    "    my_pool = my_roberta.RobertaForMaskedLM.init(Vocab, my_config, key=k_1)\n",
    "    state = my_pool.to_state_dict()\n",
    "\n",
    "    state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "    state[\"lm_head.bias\"] = torch.zeros(hf_config.vocab_size)\n",
    "\n",
    "    # print(state.keys())\n",
    "\n",
    "    hf_pool = hf_roberta.RobertaForMaskedLM(hf_config)\n",
    "    hf_pool.load_state_dict(state, strict=True, assign=True)\n",
    "\n",
    "    if ids:\n",
    "        my_output = my_pool(input_ids = input_ids, attention_mask=mask, key=k_2)\n",
    "        hf_output = hf_pool(input_ids = input_ids_torch, attention_mask=mask_torch, return_dict=False)\n",
    "    else:\n",
    "        my_output = my_pool(input_embeds = input_embeds, attention_mask=mask, key=k_2)\n",
    "        hf_output = hf_pool(inputs_embeds = input_embeds_torch, attention_mask=mask_torch, return_dict=False)\n",
    "\n",
    "    return check(my_output[0].array, hf_output[0].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = jrandom.split(key_funcs, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = []\n",
    "\n",
    "outs.append(test_RobertaSelfOutput(keys[0]))\n",
    "outs.append(test_RobertaSelfAttention(keys[1]))\n",
    "outs.append(test_RobertaAttention(keys[2]))\n",
    "outs.append(test_RobertaIntermediate(keys[3]))\n",
    "outs.append(test_RobertaOutput(keys[4]))\n",
    "outs.append(test_RobertaLayer(keys[4]))\n",
    "outs.append(test_RobertaEncoder(keys[4]))\n",
    "outs.append(test_RobertaEmbedding(keys[7], ids = True))\n",
    "outs.append(test_RobertaEmbedding(keys[8], ids = False))\n",
    "outs.append(test_RobertaModel(keys[9], ids = True, pool = True))\n",
    "outs.append(test_RobertaModel(keys[10], ids = False, pool = False))\n",
    "outs.append(test_RobertaModel(keys[9], ids = True, pool = True))\n",
    "outs.append(test_RobertaModel(keys[10], ids = False, pool = False))\n",
    "outs.append(test_RobertaPooler(keys[11]))\n",
    "outs.append(test_RobertaLMHead(keys[12]))\n",
    "outs.append(test_RobertaForMaskedLM(keys[13], ids = True))\n",
    "outs.append(test_RobertaForMaskedLM(keys[14], ids = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = [\n",
    "    \"test_RobertaSelfOutput\",\n",
    "    \"test_RobertaSelfAttention\",\n",
    "    \"test_RobertaAttention\",\n",
    "    \"test_RobertaIntermediate\",\n",
    "    \"test_RobertaOutput\",\n",
    "    \"test_RobertaLayer\",\n",
    "    \"test_RobertaEncoder\",\n",
    "    \"test_RobertaEmbedding(ids = True)\",\n",
    "    \"test_RobertaEmbedding(ids = False)\",\n",
    "    \"test_RobertaModel(ids = True, pool = True)\",\n",
    "    \"test_RobertaModel(ids = False, pool = False)\",\n",
    "    \"test_RobertaModel(ids = True, pool = True)\",\n",
    "    \"test_RobertaModel(ids = False, pool = False)\",\n",
    "    \"test_RobertaPooler\",\n",
    "    \"test_RobertaLMHead\",\n",
    "    \"test_RobertaForMaskedLM(ids = True)\",\n",
    "    \"test_RobertaForMaskedLM(ids = False)\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,o in enumerate(outs):\n",
    "    if o[2] * 0 != 0:\n",
    "        print(f\"nan alert\")\n",
    "    if o[0] < 1:\n",
    "        print(f\"{types[i]}: {o[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,o in enumerate(outs):\n",
    "    print(f\"{types[i]}: {o[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_model, key_lm = jrandom.split(key_funcs, 2)\n",
    "key_model_run, key_lm_run = jrandom.split(key_run, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias'])\n",
      "odict_keys(['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias'])\n",
      "success1\n",
      "success2\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Initializing RobertaModel\n",
    "my_model = my_roberta.RobertaModel.init(Vocab, my_config, add_pooling_layer=True, output_hidden_states=True, key=key_model)\n",
    "state_model = my_model.to_state_dict()\n",
    "\n",
    "state_model = {k: torch.from_numpy(np.array(v)) for k, v in state_model.items()}\n",
    "\n",
    "hf_model = hf_roberta.RobertaModel(hf_config, add_pooling_layer=True)\n",
    "hf_model.load_state_dict(state_model, strict=True)\n",
    "\n",
    "print(check_dicts(my_model.to_state_dict(), hf_model.state_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.embeddings.word_embeddings.weight', 'roberta.embeddings.position_embeddings.weight', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.embeddings.LayerNorm.weight', 'roberta.embeddings.LayerNorm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.bias'])\n",
      "odict_keys(['roberta.embeddings.word_embeddings.weight', 'roberta.embeddings.position_embeddings.weight', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.embeddings.LayerNorm.weight', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias'])\n",
      "success1\n",
      "success2\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Initializing RobertaForMaskedLM\n",
    "my_mlm = my_roberta.RobertaForMaskedLM.init(Vocab, my_config, output_hidden_states=True, key=key_funcs)\n",
    "state_mlm = my_mlm.to_state_dict()\n",
    "\n",
    "state_mlm = {k: torch.from_numpy(np.array(v)) for k, v in state_mlm.items()}\n",
    "state_mlm[\"lm_head.bias\"] = torch.zeros(hf_config.vocab_size)\n",
    "# print(state_mlm[w_str])\n",
    "\n",
    "hf_mlm = hf_roberta.RobertaForMaskedLM(hf_config)\n",
    "hf_mlm.load_state_dict(state_mlm, strict=True, assign=True)\n",
    "# print(hf_mlm.state_dict()[w_str])\n",
    "\n",
    "print(check_dicts(state_mlm, hf_mlm.state_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_RobertaModel_Output(key_run, ids = False):\n",
    "    if ids:\n",
    "        my_output = my_model(input_ids = input_ids, attention_mask=mask, key=key_run)\n",
    "        hf_output = hf_model(input_ids = input_ids_torch, attention_mask=mask_torch, return_dict=False, output_hidden_states=True)\n",
    "    else:\n",
    "        my_output = my_model(input_embeds = input_embeds, attention_mask=mask, key=key_run)\n",
    "        hf_output = hf_model(inputs_embeds = input_embeds_torch, attention_mask=mask_torch, return_dict=False, output_hidden_states=True)\n",
    "\n",
    "    return my_output, hf_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_output_ids, hf_output_ids = test_RobertaModel_Output(key_model_run, ids=True)\n",
    "my_output_embeds, hf_output_embeds = test_RobertaModel_Output(key_model_run, ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_out: acc: 1.0 \t norms: (tensor(888.5331), tensor(888.5331)) \t diffs: 6.946668236196274e-07\n",
      "pool_out: acc: 1.0 \t norms: (tensor(24.6133), tensor(24.6133)) \t diffs: 4.2906631847472454e-07\n",
      "intermediates:\n",
      "acc: 1.0 \t norms: (tensor(888.5315), tensor(888.5314)) \t diffs: 1.6926360046909394e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5301), tensor(888.5301)) \t diffs: 2.57225963196106e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5310), tensor(888.5311)) \t diffs: 3.1373855335914413e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5312), tensor(888.5312)) \t diffs: 3.6955742643840495e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5304), tensor(888.5304)) \t diffs: 4.1312114262836985e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5308), tensor(888.5308)) \t diffs: 4.612424220340472e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5308), tensor(888.5308)) \t diffs: 5.164657750356128e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5295), tensor(888.5295)) \t diffs: 5.412561563389318e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5309), tensor(888.5310)) \t diffs: 5.724053266931151e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5297), tensor(888.5297)) \t diffs: 5.980200512567535e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5317), tensor(888.5317)) \t diffs: 6.434746637751232e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5331), tensor(888.5331)) \t diffs: 6.946668236196274e-07\n"
     ]
    }
   ],
   "source": [
    "# RobertaModel ids\n",
    "my_out, hf_out = my_output_ids[0], hf_output_ids[0]\n",
    "\n",
    "print(f\"model_out: {check(my_out.array, hf_out.detach())[3]}\")\n",
    "\n",
    "my_pool, hf_pool = my_output_ids[1], hf_output_ids[1]\n",
    "\n",
    "print(f\"pool_out: {check(my_pool.array, hf_pool.detach())[3]}\")\n",
    "\n",
    "print(\"intermediates:\")\n",
    "my_ints, hf_ints = my_output_ids[2], hf_output_ids[2][1:]\n",
    "\n",
    "for i,j in zip(my_ints, hf_ints):\n",
    "    print(check(i.array,j.detach())[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_out: acc: 1.0 \t norms: (tensor(888.5307), tensor(888.5306)) \t diffs: 6.107513286224275e-07\n",
      "pool_out: acc: 1.0 \t norms: (tensor(24.6094), tensor(24.6094)) \t diffs: 3.8713346839358564e-07\n",
      "intermediates:\n",
      "acc: 1.0 \t norms: (tensor(888.5317), tensor(888.5317)) \t diffs: 1.3876427829018212e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5305), tensor(888.5305)) \t diffs: 2.239140144411067e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5290), tensor(888.5290)) \t diffs: 2.9642876597790746e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5301), tensor(888.5300)) \t diffs: 3.554245893155894e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5311), tensor(888.5311)) \t diffs: 4.070468264671945e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5311), tensor(888.5311)) \t diffs: 4.3890696588277933e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5305), tensor(888.5304)) \t diffs: 4.87373824853421e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5308), tensor(888.5308)) \t diffs: 5.209108735471091e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5298), tensor(888.5298)) \t diffs: 5.463080583467672e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5311), tensor(888.5311)) \t diffs: 5.576325179390551e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5295), tensor(888.5294)) \t diffs: 5.659875341734733e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5307), tensor(888.5306)) \t diffs: 6.107513286224275e-07\n"
     ]
    }
   ],
   "source": [
    "# RobertaModel embeds\n",
    "\n",
    "my_out, hf_out = my_output_embeds[0], hf_output_embeds[0]\n",
    "\n",
    "print(f\"model_out: {check(my_out.array, hf_out.detach())[3]}\")\n",
    "\n",
    "my_pool, hf_pool = my_output_embeds[1], hf_output_embeds[1]\n",
    "\n",
    "print(f\"pool_out: {check(my_pool.array, hf_pool.detach())[3]}\")\n",
    "\n",
    "print(\"intermediates:\")\n",
    "my_ints, hf_ints = my_output_embeds[2], hf_output_embeds[2][1:]\n",
    "\n",
    "for i,j in zip(my_ints, hf_ints):\n",
    "    print(check(i.array,j.detach())[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_RobertaForMaskedLM_Output(key_run, ids = False):\n",
    "    if ids:\n",
    "        my_output = my_mlm(input_ids = input_ids, attention_mask=mask, key=key_run)\n",
    "        hf_output = hf_mlm(input_ids = input_ids_torch, attention_mask=mask_torch, return_dict=False, output_hidden_states=True)\n",
    "    else:\n",
    "        my_output = my_mlm(input_embeds = input_embeds, attention_mask=mask, key=key_run)\n",
    "        hf_output = hf_mlm(inputs_embeds = input_embeds_torch, attention_mask=mask_torch, return_dict=False, output_hidden_states=True)\n",
    "\n",
    "    return my_output, hf_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mlm_output_ids, hf_mlm_output_ids = test_RobertaForMaskedLM_Output(key_run, ids=True)\n",
    "my_mlm_output_embeds, hf_mlm_output_embeds = test_RobertaForMaskedLM_Output(key_run, ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlm_out: acc: 1.0 \t norms: (tensor(7054.6812), tensor(7054.6816)) \t diffs: 7.966510224832746e-07\n",
      "intermediates:\n",
      "acc: 1.0 \t norms: (tensor(888.5315), tensor(888.5314)) \t diffs: 1.6926360046909394e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5301), tensor(888.5301)) \t diffs: 2.57225963196106e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5310), tensor(888.5311)) \t diffs: 3.1373855335914413e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5312), tensor(888.5312)) \t diffs: 3.6955742643840495e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5304), tensor(888.5304)) \t diffs: 4.1312114262836985e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5308), tensor(888.5308)) \t diffs: 4.612424220340472e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5308), tensor(888.5308)) \t diffs: 5.164657750356128e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5295), tensor(888.5295)) \t diffs: 5.412561563389318e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5309), tensor(888.5310)) \t diffs: 5.724053266931151e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5297), tensor(888.5297)) \t diffs: 5.980200512567535e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5317), tensor(888.5317)) \t diffs: 6.434746637751232e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5331), tensor(888.5331)) \t diffs: 6.946668236196274e-07\n"
     ]
    }
   ],
   "source": [
    "#Masked MLM ids\n",
    "my_out, hf_out = my_mlm_output_ids[0], hf_mlm_output_ids[0]\n",
    "\n",
    "print(f\"mlm_out: {check(my_out.array, hf_out.detach())[3]}\")\n",
    "\n",
    "print(\"intermediates:\")\n",
    "my_ints, hf_ints = my_mlm_output_ids[1], hf_mlm_output_ids[1][1:]\n",
    "\n",
    "for i,j in zip(my_ints, hf_ints):\n",
    "    print(check(i.array,j.detach(), precision = 0.01)[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlm_out: acc: 1.0 \t norms: (tensor(7107.9902), tensor(7107.9902)) \t diffs: 7.997662692105223e-07\n",
      "intermediates:\n",
      "acc: 1.0 \t norms: (tensor(888.5317), tensor(888.5317)) \t diffs: 1.3876427829018212e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5305), tensor(888.5305)) \t diffs: 2.239140144411067e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5290), tensor(888.5290)) \t diffs: 2.9642876597790746e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5301), tensor(888.5300)) \t diffs: 3.554245893155894e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5311), tensor(888.5311)) \t diffs: 4.070468264671945e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5311), tensor(888.5311)) \t diffs: 4.3890696588277933e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5305), tensor(888.5304)) \t diffs: 4.87373824853421e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5308), tensor(888.5308)) \t diffs: 5.209108735471091e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5298), tensor(888.5298)) \t diffs: 5.463080583467672e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5311), tensor(888.5311)) \t diffs: 5.576325179390551e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5295), tensor(888.5294)) \t diffs: 5.659875341734733e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5307), tensor(888.5306)) \t diffs: 6.107513286224275e-07\n"
     ]
    }
   ],
   "source": [
    "#Masked MLM embeds\n",
    "my_out, hf_out = my_mlm_output_embeds[0], hf_mlm_output_embeds[0]\n",
    "\n",
    "print(f\"mlm_out: {check(my_out.array, hf_out.detach())[3]}\")\n",
    "\n",
    "print(\"intermediates:\")\n",
    "my_ints, hf_ints = my_mlm_output_embeds[1], hf_mlm_output_embeds[1][1:]\n",
    "\n",
    "for i,j in zip(my_ints, hf_ints):\n",
    "    print(check(i.array,j.detach())[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing RobertaModel\n",
    "# my_model = my_roberta.RobertaModel.init(Vocab, my_config, add_pooling_layer=False, key=key_rob)\n",
    "# state_model = my_model.to_state_dict()\n",
    "\n",
    "# state_model = {k: torch.from_numpy(np.array(v)) for k, v in state_model.items()}\n",
    "\n",
    "# hf_model = hf_roberta.RobertaModel(hf_config, add_pooling_layer=False)\n",
    "# hf_model.load_state_dict(state_model, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['dense.weight', 'dense.bias', 'layer_norm.weight', 'layer_norm.bias', 'decoder.weight', 'decoder.bias', 'bias'])\n",
      "odict_keys(['bias', 'dense.weight', 'dense.bias', 'layer_norm.weight', 'layer_norm.bias', 'decoder.weight', 'decoder.bias'])\n",
      "success1\n",
      "success2\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Testing RobertaLMHead\n",
    "my_head = my_roberta.RobertaLMHead.init(Vocab, my_config, key=key_lm)\n",
    "state_head = my_head.to_state_dict()\n",
    "\n",
    "state_head = {k: torch.from_numpy(np.array(v)) for k, v in state_head.items()}\n",
    "state_head[\"bias\"] = torch.zeros(hf_config.vocab_size)\n",
    "\n",
    "hf_head = hf_roberta.RobertaLMHead(hf_config)\n",
    "hf_head.load_state_dict(state_head, strict=True)\n",
    "\n",
    "print(check_dicts(state_head, hf_head.state_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_output_mlm, hf_output_mlm = test_RobertaForMaskedLM_Output(key_run, ids = False)\n",
    "\n",
    "my_output_model, hf_output_model = test_RobertaModel_Output(key_model_run, ids = False)\n",
    "my_output = my_head(my_output_model[0], key=key_lm_run)\n",
    "hf_output = hf_head(hf_output_model[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k_rob, k_lm = jrandom.split(key, 2)\n",
    "\n",
    "# # MLM\n",
    "# my_output_mlm = my_mlm(input_embeds = input_embeds, attention_mask=mask, key=key)\n",
    "# hf_output_mlm = hf_mlm(inputs_embeds = input_embeds_torch, attention_mask=mask_torch, return_dict=False)\n",
    "\n",
    "# # Model + LM\n",
    "\n",
    "# my_output_model = my_model(input_embeds = input_embeds, attention_mask=mask, key=k_rob)\n",
    "# my_output = my_head(my_output_model[0], key=k_lm)\n",
    "\n",
    "# hf_output_model = hf_model(inputs_embeds = input_embeds_torch, attention_mask=mask_torch, return_dict=False)\n",
    "# hf_output = hf_head(hf_output_model[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaModel: acc: 1.0 \t norms: (tensor(888.5307), tensor(888.5306)) \t diffs: 6.107513286224275e-07\n",
      "Roberta Model + LM head: acc: 1.0 \t norms: (tensor(7107.9902), tensor(7107.9902)) \t diffs: 7.997662692105223e-07\n",
      "MLM: acc: 1.0 \t norms: (tensor(7107.9902), tensor(7107.9902)) \t diffs: 7.997662692105223e-07\n",
      "my RobertaModel + LM head vs MLM: acc: 1.0 \t norms: (tensor(7107.9902), tensor(7107.9902)) \t diffs: 0.0\n",
      "hf RobertaModel + LM head vs MLM: acc: 1.0 \t norms: (tensor(7107.9902), tensor(7107.9902)) \t diffs: 0.0\n"
     ]
    }
   ],
   "source": [
    "# embeds\n",
    "print(f\"RobertaModel: {check(my_output_model[0].array, hf_output_model[0].detach())[3]}\")\n",
    "print(f\"Roberta Model + LM head: {check(my_output.array, hf_output.detach())[3]}\")\n",
    "print(f\"MLM: {check(my_output_mlm[0].array, hf_output_mlm[0].detach())[3]}\")\n",
    "\n",
    "print(f\"my RobertaModel + LM head vs MLM: {check(my_output.array, my_output_mlm[0].array)[3]}\")\n",
    "print(f\"hf RobertaModel + LM head vs MLM: {check(hf_output.detach(), hf_output_mlm[0].detach())[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_output_mlm, hf_output_mlm = test_RobertaForMaskedLM_Output(key_run, ids = True)\n",
    "\n",
    "my_output_model, hf_output_model = test_RobertaModel_Output(key_model_run, ids = True)\n",
    "my_output = my_head(my_output_model[0], key=key_lm_run)\n",
    "hf_output = hf_head(hf_output_model[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MLM\n",
    "# my_output_mlm = my_mlm(input_ids = input_ids, attention_mask=mask, key=key)\n",
    "# hf_output_mlm = hf_mlm(input_ids = input_ids_torch, attention_mask=mask_torch, return_dict=False)\n",
    "\n",
    "# # Model + LM\n",
    "# my_output_model = my_model(input_ids = input_ids, attention_mask=mask, key=k_rob)\n",
    "# my_output = my_head(my_output_model[0], key=k_lm)\n",
    "\n",
    "# hf_output_model = hf_model(input_ids = input_ids_torch, attention_mask=mask_torch, return_dict=False)\n",
    "# hf_output = hf_head(hf_output_model[0])\n",
    "\n",
    "# # Notes\n",
    "# # embeds works between hf and my, and within model->head and mlm \n",
    "# # ids does not work between hf and my for mlm or within hf for model->head and mlm - so hf mlm is doing something weird.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaModel: acc: 1.0 \t norms: (tensor(888.5331), tensor(888.5331)) \t diffs: 6.946668236196274e-07\n",
      "Roberta Model + LM head: acc: 1.0 \t norms: (tensor(7054.6812), tensor(7054.6816)) \t diffs: 7.966510224832746e-07\n",
      "MLM: acc: 1.0 \t norms: (tensor(7054.6812), tensor(7054.6816)) \t diffs: 7.966510224832746e-07\n",
      "my RobertaModel + LM head vs MLM: acc: 1.0 \t norms: (tensor(7054.6812), tensor(7054.6812)) \t diffs: 0.0\n",
      "hf RobertaModel + LM head vs MLM: acc: 1.0 \t norms: (tensor(7054.6816), tensor(7054.6816)) \t diffs: 0.0\n"
     ]
    }
   ],
   "source": [
    "# ids\n",
    "print(f\"RobertaModel: {check(my_output_model[0].array, hf_output_model[0].detach())[3]}\")\n",
    "print(f\"Roberta Model + LM head: {check(my_output.array, hf_output.detach())[3]}\")\n",
    "print(f\"MLM: {check(my_output_mlm[0].array, hf_output_mlm[0].detach())[3]}\")\n",
    "\n",
    "print(f\"my RobertaModel + LM head vs MLM: {check(my_output.array, my_output_mlm[0].array)[3]}\")\n",
    "print(f\"hf RobertaModel + LM head vs MLM: {check(hf_output.detach(), hf_output_mlm[0].detach())[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mstop\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained weights from hf\n",
    "hf_model = hf_roberta.RobertaModel.from_pretrained(\"roberta-base\")\n",
    "state_model = hf_model.state_dict()\n",
    "\n",
    "state_model = {k: np.array(v) for k, v in state_model.items()}\n",
    "\n",
    "hf_config = hf_model.config\n",
    "hf_config.hidden_dropout_prob = 0\n",
    "hf_config.attention_probs_dropout_prob = 0\n",
    "my_config = my_roberta.RobertaConfig.from_hf_config(hf_config)\n",
    "\n",
    "my_model = my_roberta.RobertaModel.init(Vocab, my_config, output_hidden_states=True, key=key)\n",
    "my_model = my_model.from_state_dict(state_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias'])\n",
      "odict_keys(['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias'])\n",
      "success1\n",
      "success2\n",
      "Total differences: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Check weights loaded correctly\n",
    "my_dict = my_model.to_state_dict()\n",
    "hf_dict = hf_model.state_dict()\n",
    "\n",
    "print(f\"Total differences: {check_dicts(my_dict, hf_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "my_output_model = my_model(input_embeds = input_embeds, attention_mask=mask, key=key)\n",
    "hf_output_model = hf_model(inputs_embeds = input_embeds_torch, attention_mask=mask_torch, return_dict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: acc: 1.0 \t norms: (tensor(443.3569), tensor(443.3564)) \t diffs: 1.405485477334878e-06\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model: {check(my_output_model[0].array, hf_output_model[0].detach())[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "my_output_model = my_model(input_ids = input_ids, attention_mask=mask, key=key)\n",
    "hf_output_model = hf_model(input_ids = input_ids_torch, attention_mask=mask_torch, return_dict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: acc: 1.0 \t norms: (tensor(431.9545), tensor(431.9545)) \t diffs: 1.3033360346526024e-06\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model: {check(my_output_model[0].array, hf_output_model[0].detach())[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_mlm = hf_roberta.RobertaForMaskedLM.from_pretrained(hf_model_str)\n",
    "state_mlm = hf_mlm.state_dict()\n",
    "\n",
    "state_mlm = {k: np.array(v) for k, v in state_mlm.items()}\n",
    "\n",
    "hf_config = hf_mlm.config\n",
    "hf_config.hidden_dropout_prob = 0\n",
    "hf_config.attention_probs_dropout_prob = 0\n",
    "my_config = my_roberta.RobertaConfig.from_hf_config(hf_config)\n",
    "\n",
    "my_mlm = my_roberta.RobertaForMaskedLM.init(Vocab, my_config, key=key)\n",
    "my_mlm = my_mlm.from_state_dict(state_mlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.embeddings.word_embeddings.weight', 'roberta.embeddings.position_embeddings.weight', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.embeddings.LayerNorm.weight', 'roberta.embeddings.LayerNorm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias'])\n",
      "odict_keys(['roberta.embeddings.word_embeddings.weight', 'roberta.embeddings.position_embeddings.weight', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.embeddings.LayerNorm.weight', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias'])\n",
      "success1\n",
      "fail2\n",
      "['lm_head.bias']\n",
      "Total differences: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Check weights loaded correctly\n",
    "my_dict = my_mlm.to_state_dict()\n",
    "hf_dict = hf_mlm.state_dict()\n",
    "\n",
    "print(f\"Total differences: {check_dicts(my_dict, hf_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0972, -0.0294,  0.4988,  ..., -0.0312, -0.0312, -1.0000])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dict['lm_head.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0972, -0.0294,  0.4988,  ..., -0.0312, -0.0312, -1.0000])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dict['lm_head.decoder.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "my_output_mlm = my_mlm(input_embeds = input_embeds, attention_mask=mask, key=key)\n",
    "hf_output_mlm = hf_mlm(inputs_embeds = input_embeds_torch, attention_mask=mask_torch, return_dict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLM: acc: 1.0 \t norms: (tensor(33433.4062), tensor(33433.3945)) \t diffs: 1.7561978893354535e-05\n"
     ]
    }
   ],
   "source": [
    "print(f\"MLM: {check(my_output_mlm[0].array, hf_output_mlm[0].detach())[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "my_output_mlm = my_mlm(input_ids = input_ids, attention_mask=mask, key=key)\n",
    "hf_output_mlm = hf_mlm(input_ids = input_ids_torch, attention_mask=mask_torch, return_dict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLM: acc: 1.0 \t norms: (tensor(28814.2480), tensor(28814.2168)) \t diffs: 1.45375252031954e-05\n"
     ]
    }
   ],
   "source": [
    "print(f\"MLM: {check(my_output_mlm[0].array, hf_output_mlm[0].detach())[3]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "levanter2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
