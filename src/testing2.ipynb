{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig as HFRobertaConfig\n",
    "from transformers import RobertaForMaskedLM as HFRobertaForMaskedLM\n",
    "from levanter.models.roberta import RobertaConfig\n",
    "from levanter.models.roberta import RobertaForMaskedLM\n",
    "\n",
    "import jax\n",
    "import jax.random as jrandom\n",
    "import jax.numpy as jnp\n",
    "import haliax as hax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 203/203 [00:01<00:00, 148.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab(50265)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\partitioning.py:116: RuntimeWarning: No resource mapping found. Not sharding.\n",
      "  warnings.warn(\"No resource mapping found. Not sharding.\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "def load_weights_from_hf():\n",
    "    # Load the Hugging Face model\n",
    "    hf_model = HFRobertaForMaskedLM.from_pretrained(\"roberta-base\")\n",
    "    hf_config = HFRobertaConfig.from_pretrained(\"roberta-base\")\n",
    "\n",
    "    hf_config.hidden_dropout_prob = 0\n",
    "    hf_config.attention_probs_dropout_prob = 0\n",
    "    # hf_config.pad_token_id = -1\n",
    "\n",
    "    lv_config = RobertaConfig.from_hf_config(hf_config)\n",
    "\n",
    "    converter = lv_config.hf_checkpoint_converter()\n",
    "\n",
    "    model = converter.load_pretrained(\n",
    "        lv_config.model_type,\n",
    "        lv_config,\n",
    "        axis_mapping=None, \n",
    "        dtype=\"float32\",  \n",
    "    )\n",
    "\n",
    "    print(converter.Vocab)\n",
    "    \n",
    "    #print(\"Weights loaded successfully.\")\n",
    "    return model, lv_config, hf_model, hf_config\n",
    "\n",
    "lv_model, lv_config, hf_model, hf_config = load_weights_from_hf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50265\n",
      "50265\n",
      "514\n",
      "514\n"
     ]
    }
   ],
   "source": [
    "print(lv_config.vocab_size)\n",
    "print(hf_config.vocab_size)\n",
    "\n",
    "print(lv_config.max_position_embeddings)\n",
    "print(hf_config.max_position_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def named_to_tensor(named_array):\n",
    "    out_tensor = torch.tensor(np.array(named_array.array))\n",
    "    return out_tensor\n",
    "\n",
    "def tensor_to_named(in_tensor, axes):\n",
    "    named_array = hax.NamedArray(np.array(in_tensor), axes)\n",
    "    return named_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare outputs\n",
    "def check(my_out, hf_out, precision=1e-4):\n",
    "    acc = np.isclose(hf_out, my_out, rtol=precision, atol=precision).mean()\n",
    "    diff = np.abs(my_out - hf_out).mean()\n",
    "    return f\"Accuracy: {acc:.4f}, Avg Difference: {diff:.6f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch = hax.Axis(\"batch\", 1)\n",
    "Pos = lv_config.Pos\n",
    "KeyPos = lv_config.KeyPos\n",
    "Vocab = hax.Axis(\"vocab\", tokenizer.vocab_size)\n",
    "\n",
    "key = jrandom.PRNGKey(42)\n",
    "key_var, key_model = jrandom.split(key, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (514 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "prompt = tokenizer(\"Explaining metaphysics to the nation. I wish he would explain his explanation. You, Bob, are rather insolent, you know, At being disappointed in your wish To supersede all warblers here below, And be the only blackbird in the dish. And then you overstrain yourself, or so And tumble downward like the flying fish Gasping on deck, because you soar too high, Bob, And fall for lack of moisture quite a dry Bob. And Wordsworth in a rather long Excursion (I think the quarto holds five hundred pages) Has given a sample from the vasty version Of his new system to perplex the sages. ’Tis poetry, at least by his assertion, And may appear so when the Dog Star rages, And he who understands it would be able To add a story to the tower of Babel. You gentlemen, by dint of long seclusion From better company, have kept your own At Keswick, and through still continued fusion Of one another’s minds at last have grown To deem, as a most logical conclusion, That poesy has wreaths for you alone. There is a narrowness in such a notion, Which makes me wish you’d change your lakes for ocean. I would not imitate the petty thought, Nor coin my self-love to so base a vice,  For all the glory your conversion brought,    Since gold alone should not have been its price. You have your salary; was’t for that you wrought?    And Wordsworth has his place in the Excise. You’re shabby fellows—true—but poets still And duly seated on the immortal hill. Your bays may hide the baldness of your brows,    Perhaps some virtuous blushes; let them go. To you I envy neither fruit nor boughs,  And for the fame you would engross below, The field is universal and allows    Scope to all such as feel the inherent glow. Scott, Rogers, Campbell, Moore, and Crabbe will try ’Gainst you the question with posterity. For me, who, wandering with pedestrian Muses,    Contend not with you on the winged’ steed, I wish your fate may yield ye, when she chooses,    The fame you envy and the skill you need. And recollect a poet nothing loses    In giving to his    \", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = tokenizer(\"Mark <mask> is the CEO of Facebook, located in <mask> <mask>, California.\", return_tensors=\"pt\", padding='max_length', max_length=514)\n",
    "# prompt = tokenizer(\"Paris is the <mask> of France.\", return_tensors=\"pt\", padding='max_length', max_length=514)\n",
    "# prompt = tokenizer(\"Mark Zuckerberg is the boss of Facebook, located in Palo Alto, California.\", return_tensors=\"pt\", padding='max_length', max_length=514)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "lv_prompt = {\"input_ids\": tensor_to_named(prompt[\"input_ids\"], (Batch, Pos)), \"attention_mask\": tensor_to_named(prompt[\"attention_mask\"], (Batch, KeyPos))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention_mask_tensor = torch.ones(size = (Batch.size, Pos.size), dtype = int)\n",
    "# prompt[\"attention_mask\"] = attention_mask_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lv_prompt = {k: hax.NamedArray(np.array((prompt[k])), axes = (Batch, Pos)) for k in prompt.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = hax.random.randint(key_var, shape = (Batch, Pos), minval = lv_config.eos_token_id+1, maxval = lv_config.vocab_size)\n",
    "# attention_mask = hax.ones(shape = (Batch, Pos), dtype=int)\n",
    "\n",
    "# input_ids_tensor = named_to_tensor(input_ids)\n",
    "# attention_mask_tensor = named_to_tensor(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = haliax.random.randint(jax.random.PRNGKey(42), shape = (haliax.Axis(\"batch\", 1), haliax.Axis(\"position\", 512)), minval = 4, maxval = 50265)\n",
    "# input_ids = torch.ones((1, 512), dtype=int) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids_tensor = torch.randint(low = lv_config.eos_token_id+1, high = lv_config.vocab_size, size = (Batch.size, Pos.size))\n",
    "# # input_ids_tensor[:, 0] = 0\n",
    "# # input_ids_tensor[:, (lv_config.max_position_embeddings // 2):] = 1\n",
    "# # input_ids_tensor[:, (lv_config.max_position_embeddings // 2)] = 2\n",
    "\n",
    "# # # input_ids_tensor = torch.ones((Batch.size, Pos.size), dtype=int) * 100\n",
    "\n",
    "# idx = torch.randint(low = 1, high=lv_config.max_position_embeddings, size = (1,))\n",
    "# # idx = lv_config.max_position_embeddings // 2\n",
    "\n",
    "# attention_mask_tensor = torch.ones(size = (Batch.size, KeyPos.size), dtype = int)\n",
    "# # attention_mask_tensor[:, idx:] = 0\n",
    "\n",
    "# attention_mask = tensor_to_named(attention_mask_tensor, (Batch, KeyPos))\n",
    "\n",
    "# # # attention_mask_tensor = torch.ones(size = (Batch.size, Pos.size), dtype = int)\n",
    "# # # attention_mask_tensor[:, idx:] = 0\n",
    "\n",
    "# # # attention_mask = tensor_to_named(attention_mask_tensor, (Batch, KeyPos))\n",
    "\n",
    "# input_ids = tensor_to_named(input_ids_tensor, (Batch, Pos))\n",
    "# prompt = {\"input_ids\": input_ids_tensor, \"attention_mask\": attention_mask_tensor}\n",
    "# lv_prompt = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "\n",
    "# # # prompt = {\"input_ids\": input_ids_tensor}\n",
    "# # # lv_prompt = {\"input_ids\": input_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = {\"input_ids\": input_ids_tensor, \"attention_mask\": attention_mask_tensor}\n",
    "# lv_prompt = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_position_ids_from_input_ids(input_ids, past_key_values_length=0):\n",
    "#     mask = hax.not_equal(input_ids, lv_config.pad_token_id) * 1\n",
    "#     incremental_indices = (hax.cumsum(mask, axis=lv_config.Pos).astype(mask) + past_key_values_length) * mask\n",
    "#     incremental_indices -= mask.all(axis=Pos)\n",
    "#     return incremental_indices + lv_config.pad_token_id\n",
    "\n",
    "# def create_position_ids_from_input_ids(input_ids, past_key_values_length=0):\n",
    "#     return hax.arange(axis = Pos, start = 0, dtype=jnp.int32)\n",
    "\n",
    "def create_position_ids_from_input_ids(input_ids, PosInput, past_key_values_length=0):\n",
    "    mask = hax.not_equal(input_ids, lv_config.pad_token_id) * 1\n",
    "    incremental_indices = (hax.cumsum(mask, axis=PosInput).astype(mask) + past_key_values_length) * mask\n",
    "    incremental_indices -= mask.all(axis=PosInput) * lv_config.pad_token_id\n",
    "    return incremental_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict = hf_model.state_dict()\n",
    "# print(dict[\"roberta.embeddings.position_embeddings.weight\"].shape)\n",
    "# print(dict[\"roberta.embeddings.position_embeddings.weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_ids = create_position_ids_from_input_ids(lv_prompt[\"input_ids\"], Pos)\n",
    "\n",
    "lv_prompt[\"position_ids\"] = position_ids\n",
    "prompt[\"position_ids\"] = torch.from_numpy(np.array(position_ids.array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000, Avg Difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "print(check(np.array(lv_prompt[\"input_ids\"].array), np.array(prompt[\"input_ids\"])))\n",
    "# print(check(np.array(lv_prompt[\"attention_mask\"].array), np.array(prompt[\"attention_mask\"])))\n",
    "# print(check(np.array(lv_prompt[\"position_ids\"].array), np.array(prompt[\"position_ids\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_dicts(my_dict, hf_dict):\n",
    "#     print(my_dict.keys())\n",
    "#     print(hf_dict.keys())\n",
    "\n",
    "#     my_keys = set(my_dict)\n",
    "#     hf_keys = set(hf_dict)\n",
    "\n",
    "#     both = hf_keys.union(my_keys)\n",
    "\n",
    "#     correct_msg = \"Accuracy: 1.0000, Avg Difference: 0.000000\"\n",
    "\n",
    "#     for k in both:\n",
    "#         if k not in my_keys:\n",
    "#             print(f\"Key {k} not in my_keys!\")\n",
    "#             continue\n",
    "\n",
    "#         if k not in hf_keys:\n",
    "#             print(f\"Key {k} not in hf_keys!\")\n",
    "#             continue\n",
    "        \n",
    "#         check_str = check(my_dict[k], hf_dict[k])\n",
    "#         if check_str != correct_msg:\n",
    "#             print(check_str)\n",
    "\n",
    "# my_dict = lv_model.to_state_dict()\n",
    "# hf_dict = hf_model.state_dict()\n",
    "\n",
    "# my_dict = {k: np.array(my_dict[k]) for k in my_dict.keys()}\n",
    "# hf_dict = {k: np.array(hf_dict[k]) for k in hf_dict.keys()}\n",
    "\n",
    "# check_dicts(my_dict, hf_dict)\n",
    "\n",
    "# print(check(my_dict[\"lm_head.decoder.bias\"], hf_dict[\"lm_head.decoder.bias\"]))\n",
    "# print(check(my_dict[\"lm_head.decoder.bias\"], hf_dict[\"lm_head.bias\"]))\n",
    "# print(check(hf_dict[\"lm_head.decoder.bias\"], hf_dict[\"lm_head.bias\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(hf_config.bos_token_id)\n",
    "print(hf_config.pad_token_id)\n",
    "print(hf_config.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_embeddings = hf_dict[\"roberta.embeddings.word_embeddings.weight\"]\n",
    "# input_ids = lv_prompt[\"input_ids\"]\n",
    "# input_ids_np = np.array(lv_prompt[\"input_ids\"].array)\n",
    "\n",
    "# input_embeds_np = word_embeddings[input_ids_np]\n",
    "# input_embeds_hax = hax.NamedArray(input_embeds_np, axes=(Batch, Pos, lv_config.Embed))\n",
    "\n",
    "# cond = (input_ids == lv_config.pad_token_id)\n",
    "# cond = hax.broadcast_to(cond, input_embeds_hax.axes)\n",
    "\n",
    "# input_embeds_hax = hax.where(\n",
    "#     cond,\n",
    "#     hax.zeros_like(input_embeds_hax),\n",
    "#     input_embeds_hax\n",
    "# )\n",
    "\n",
    "# input_embeds_torch = torch.from_numpy(np.array(input_embeds_hax.array))\n",
    "# new_lv_prompt = dict()\n",
    "# new_hf_prompt = dict()\n",
    "\n",
    "# new_lv_prompt[\"input_embeds\"] = input_embeds_hax\n",
    "# new_hf_prompt[\"inputs_embeds\"] = input_embeds_torch\n",
    "\n",
    "# new_lv_prompt[\"attention_mask\"] = lv_prompt[\"attention_mask\"]\n",
    "# new_hf_prompt[\"attention_mask\"] = prompt[\"attention_mask\"]\n",
    "\n",
    "# new_lv_prompt[\"position_ids\"] = lv_prompt[\"position_ids\"]\n",
    "# new_hf_prompt[\"position_ids\"] = prompt[\"position_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = hax.Axis(\"batch\", 1)\n",
    "# heads = hax.Axis(\"heads\", 12)\n",
    "# position = hax.Axis(\"position\", 514)\n",
    "# key_position = hax.Axis(\"key_position\", 514)\n",
    "\n",
    "# attention_scores = hax.ones({batch: batch.size, heads: heads.size, position: position.size, key_position: key_position.size})\n",
    "\n",
    "# attention_mask = lv_prompt[\"attention_mask\"]\n",
    "\n",
    "# attention_mask = (attention_mask == 0) * -1e12\n",
    "\n",
    "# attention_mask = attention_mask.rename({\"position\": \"key_position\"})\n",
    "# # print(attention_mask)\n",
    "# print(attention_mask.axes)\n",
    "\n",
    "# attention = attention_scores + attention_mask\n",
    "\n",
    "# print(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "lv_result = lv_model(**lv_prompt)\n",
    "hf_result = hf_model(**prompt)\n",
    "\n",
    "# lv_result = lv_model(**new_lv_prompt)\n",
    "# hf_result = hf_model(**new_hf_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "lv_result_logits = torch.from_numpy(np.array(lv_result.array))\n",
    "hf_result_logits = hf_result.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lv_logits vs hf_logits: Accuracy: 1.0000, Avg Difference: 0.000005\n",
      "lv_tokens vs hf_tokens: Accuracy: 1.0000, Avg Difference: 0.000000\n",
      "Accuracy: 0.9689, Avg Difference: 538.085603\n",
      "Accuracy: 0.9689, Avg Difference: 538.085603\n"
     ]
    }
   ],
   "source": [
    "print(f\"lv_logits vs hf_logits: {check(np.array(lv_result_logits), np.array(hf_result_logits.detach()))}\")\n",
    "print(f\"lv_tokens vs hf_tokens: {check(np.array(lv_result_logits.argmax(dim=-1)), np.array(hf_result_logits.argmax(dim=-1).detach()))}\")\n",
    "print(check(np.array(lv_result_logits.argmax(dim=-1)), np.array(prompt[\"input_ids\"])))\n",
    "print(check(np.array(hf_result_logits.argmax(dim=-1).detach()), np.array(prompt[\"input_ids\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from levanter.models.lm_model import MaskedLmExample\n",
    "\n",
    "# example = MaskedLmExample.masked_lm(tokens=lv_result.argmax(\"vocab\")[{\"batch\": 0}], targets=lv_prompt[\"input_ids\"][{\"batch\": 0}], mask_token_id=tokenizer.mask_token_id, attn_mask=lv_prompt[\"attention_mask\"][{\"batch\": 0}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NamedArray(array=Array(3.7537427, dtype=float32), axes=())"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from levanter.models.lm_model import MaskedLmExample\n",
    "\n",
    "mask_prob = 0.15\n",
    "mask_token_id = tokenizer.mask_token_id\n",
    "noise_prob = 0.1\n",
    "\n",
    "QPos = Pos\n",
    "KPos = KeyPos\n",
    "\n",
    "def _create_mlm_example(tokens, key):\n",
    "    tokens_array = tokens.array\n",
    "    targets = tokens_array.copy()\n",
    "\n",
    "    if mask_prob > 0:\n",
    "        this_key, key = jax.random.split(key)\n",
    "        mask_shape = tokens_array.shape\n",
    "        mask = jax.random.bernoulli(this_key, mask_prob, mask_shape)\n",
    "\n",
    "        rand = jax.random.uniform(this_key, mask_shape)\n",
    "        mask_token = jnp.where(rand < 0.8, mask_token_id, tokens_array)\n",
    "        random_tokens = jax.random.randint(this_key, mask_shape, 0, tokens_array.max() + 1)\n",
    "        mask_token = jnp.where((rand >= 0.8) & (rand < 0.8 + noise_prob), random_tokens, mask_token)\n",
    "        masked_tokens = jnp.where(mask, mask_token, tokens_array)\n",
    "\n",
    "        # Set targets to the original tokens where mask is True, otherwise set to mask_token_id\n",
    "        targets = jnp.where(mask, tokens_array, mask_token_id)\n",
    "\n",
    "        masked_tokens_named = hax.named(masked_tokens, QPos)\n",
    "        targets_named = hax.named(targets, QPos)\n",
    "\n",
    "        attn_mask_shape = (tokens_array.shape[0], tokens_array.shape[0])\n",
    "        attn_mask = hax.named(jnp.ones(attn_mask_shape, dtype=jnp.bool_), (QPos, KPos))\n",
    "\n",
    "        example = MaskedLmExample.masked_lm(tokens=masked_tokens_named, targets=targets_named, mask_token_id=mask_token_id, attn_mask=attn_mask)\n",
    "    else:\n",
    "        targets_named = hax.named(targets, QPos)\n",
    "        attn_mask_shape = (tokens_array.shape[0], tokens_array.shape[0])\n",
    "        attn_mask = hax.named(jnp.ones(attn_mask_shape, dtype=jnp.bool_), (QPos, KPos))\n",
    "\n",
    "        example = MaskedLmExample.masked_lm(tokens=tokens, targets=targets_named, mask_token_id=mask_token_id, attn_mask=attn_mask)\n",
    "\n",
    "    return example\n",
    "\n",
    "example = _create_mlm_example(lv_prompt[\"input_ids\"][{\"batch\": 0}], key_model)\n",
    "lv_model.compute_loss(example, key=key_model, reduction=hax.mean, reduction_axis=Pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check(named_to_tensor(example.tokens * 1.), named_to_tensor(lv_prompt[\"input_ids\"]* 1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.decode(named_to_tensor(example.tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from haliax.nn import cross_entropy_loss\n",
    "# import torch.types\n",
    "\n",
    "# lv_logits = lv_model(example.tokens, example.attn_mask, position_ids=lv_prompt[\"position_ids\"], key=key)\n",
    "# lv_logits = lv_logits.astype(jnp.float32)\n",
    "\n",
    "# hf_logits = hf_model(**{\"input_ids\" : named_to_tensor(example.tokens)[None, :], \"attention_mask\" : named_to_tensor(example.attn_mask)[None, :], \"position_ids\" : prompt[\"position_ids\"]}).logits\n",
    "# hf_logits = tensor_to_named(hf_logits.detach().numpy(), (Batch, Pos, Vocab))\n",
    "\n",
    "# targets = example.targets\n",
    "# target_y = hax.nn.one_hot(targets, Vocab, dtype=lv_logits.dtype)\n",
    "\n",
    "# # jnp.dtype(\"long\")\n",
    "\n",
    "# lv_on_lv_loss = cross_entropy_loss(\n",
    "#     lv_logits, Vocab, target_y, hax.mean, reduction_axis=Pos, where=example.loss_mask\n",
    "# )\n",
    "\n",
    "# hf_on_lv_loss = cross_entropy_loss(\n",
    "#     hf_logits, Vocab, target_y, hax.mean, reduction_axis=Pos, where=example.loss_mask\n",
    "# )\n",
    "\n",
    "# # lv_loss = cross_entropy_loss(\n",
    "# #     logits, Vocab, target_y, hax.mean, reduction_axis=Pos\n",
    "# # )\n",
    "# # target_y_torch = named_to_tensor(target_y)\n",
    "\n",
    "# targets_torch = named_to_tensor(example.targets)\n",
    "# # targets_torch = named_to_tensor(example.targets.astype(jnp.dtype(\"longlong\")))\n",
    "\n",
    "# targets_torch = targets_torch * named_to_tensor(example.loss_mask)\n",
    "# targets_torch[targets_torch == 0] = -100\n",
    "# targets_torch = torch.tensor(targets_torch, dtype=torch.long)\n",
    "\n",
    "# lv_on_torch_loss = torch.nn.functional.cross_entropy(named_to_tensor(lv_logits)[0], targets_torch)\n",
    "# hf_on_torch_loss = torch.nn.functional.cross_entropy(named_to_tensor(hf_logits)[0], targets_torch)\n",
    "\n",
    "# print(lv_on_lv_loss)\n",
    "# print(hf_on_lv_loss)\n",
    "# print(lv_on_torch_loss)\n",
    "# print(hf_on_torch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     2,     0, 17297,  2633,     7,     5,  1226,     4,    38,\n",
       "          2813,    37,    74,  3922,    39,  8257,     4,   370,     6,  3045,\n",
       "             6,    32,  1195, 23799,  1342,     6,    47,   216,     6,   497,\n",
       "           145,  5779,    11,   110,  2813,   598, 31716, 12820,    70,   997,\n",
       "         25274,   259,   874,     6,   178,    28,     5,   129,   909, 15886,\n",
       "            11,     5,  8847,     4,   178,   172,    47,    81,  6031,  1851,\n",
       "          2512,     6,    50,    98,   178, 26566, 14659,   101,     5,  4731,\n",
       "          3539,   272,  9331,   154,    15,  9124,     6,   142,    47, 27283,\n",
       "           350,   239,     6,  3045,     6,   178,  1136,    13,  1762,     9,\n",
       "         16227, 27800,    10,  3841,  3045,     4,   178, 15690, 15142,    11,\n",
       "            10,  1195,   251, 22847, 29471,    36,   100,   206,     5, 18284,\n",
       "           139,  3106,   292,  6317,  6052,    43,  6233,   576,    10,  7728,\n",
       "            31,     5, 18940,   219,  1732,  1525,    39,    92,   467,     7,\n",
       "         33708,     5,   579,  3443,     4,    44,    27,   565,   354, 14665,\n",
       "             6,    23,   513,    30,    39, 32809,     6,   178,   189,  2082,\n",
       "            98,    77,     5,  8563,  2141,   910,  3443,     6,   178,    37,\n",
       "            54,  8832,    24,    74,    28,   441,   598,  1606,    10,   527,\n",
       "             7,     5,  9368,     9, 46060,     4,   370, 22629,     6,    30,\n",
       "           385,  2544,     9,   251,   842, 27953,  1740,   357,   138,     6,\n",
       "            33,  1682,   110,   308,   497, 18987,  8030,     6,     8,   149,\n",
       "           202,  1143, 24904,  1525,    65,   277,    17,    27,    29,  7283,\n",
       "            23,    94,    33,  3831,   598, 27907,     6,    25,    10,   144,\n",
       "         16437,  6427,     6,   280,  4202, 21262,    34,   885, 12353, 15354,\n",
       "            13,    47,  1937,     4,   345,    16,    10, 34139, 19649,  3361,\n",
       "            11,   215,    10,  9976,     6,  6834,   817,   162,  2813,    47,\n",
       "            17,    27,   417,   464,   110,  1508,    13,   162,     4,    38,\n",
       "            74,    45, 41379,     5, 25070,   802,     6,  6567, 12911,   127,\n",
       "          1403,    12, 17693,     7,    98, 25070,    10,  2626,     6,  1437,\n",
       "           286,    70,     5, 12594,   110, 10012,  1146,     6,  1437,  1437,\n",
       "          1437,  1773,  1637,  1937,   197,    45,    33,    57,    63,   425,\n",
       "             4,   370,    33,   110,  5391,   131,   938,    17,    27,    90,\n",
       "            13,    14,    47, 41890,   116,  1437,  1437,  1437,   178, 15690,\n",
       "         15142,    34,    39,   317,    11,     5, 22847,  1496,     4,   370,\n",
       "            17,    27,   241,  1481, 35878, 36304,   578, 29225,   578,  4297,\n",
       "         33297,   202,   178, 27069, 17630,    15,     5, 33427,  9910,     4,\n",
       "          2486,   741,  4113,   189,  7433,     5, 24876,  1825,     9,   110,\n",
       "         27423,    29,     6,  1437,  1437,  1437,  6259,   103, 41890,  3089,\n",
       "         27843,   131,   905,   106,   213,     4,   598,    47,    38, 29778,\n",
       "          5063,  6231,  3486,   741,  4894,    29,     6,  1437,   178,    13,\n",
       "             5,  9444,    47,    74, 20407, 14500,   874,     6,    20,   882,\n",
       "            16, 10547,     8,  2386,  1437,  1437,  1437,    85,     7,    70,\n",
       "           215,    25,   619,     5, 17886, 20059,     4,  1699,     6,  7541,\n",
       "             6,  5925,     6,  3404,     6,     8, 25908,  1610,    40,   860,\n",
       "            44,    27,   534,  1851,     7,    47,     5,   864,    19, 11566,\n",
       "          1571,     4,   286,   162,     6,    54,     6, 26884,    19,     5,\n",
       "          5950,   293,     6,  1437,  1437,  1437,  7555,  1397,    45,    19,\n",
       "            47,    15,     5,  5897,   196,    17,    27,  1690, 10247,     6,\n",
       "            38,  2813,   110,  7658,   189,  3363, 32440,     6,    77,    79,\n",
       "         19662,     6,  1437,  1437,  1437,    20,  9444,    47, 29778,     8,\n",
       "             5,  6707,    47,   240,     4,   178, 37941, 17601,    10, 16893,\n",
       "          1085, 13585,  1437,  1437,  1437,    96,  1311,     7,    39,  1437,\n",
       "          1437,  1437,  1437,     2]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lv_result_logits.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_fn = ['</s>Markham is the city of California, located in San Diego, California</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>']\n",
    "\n",
    "no_ids = ['</s>Markham is the city of California, located in San Diego, California</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>']\n",
    "\n",
    "arange = [\"</s>Mark</s> is the CEO of Facebook, located in San Angeles, California</s></s> cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast</s> cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast</s> cast</s></s></s></s></s></s></s></s> cast cast cast cast</s></s></s></s></s></s></s></s> cast</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s> cast cast</s> cast cast cast</s></s></s></s></s></s> cast cast cast cast</s></s></s></s></s></s></s></s> cast</s> cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast</s> cast cast</s></s></s> cast</s></s></s></s></s></s></s></s></s></s></s></s></s></s> cast cast</s></s></s></s></s></s> chance cast cast cast cast cast</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s> cast cast cast cast cast cast</s></s></s></s></s></s></s></s> cast</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s> cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast</s> cast cast cast cast cast cast cast</s> cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast</s></s></s></s></s> chance cast cast cast cast cast cast cast cast chance cast cast cast cast cast cast</s> cast</s></s></s></s></s></s></s></s></s></s></s></s></s></s> cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast chance chance cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast</s></s></s></s></s> cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast cast</s></s></s></s></s></s></s></s></s> cast chance chance cast cast cast</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s> chance cast</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s> cast cast cast cast cast cast cast cast chance chance</s>\"]\n",
    "\n",
    "arange_no_offset = \"nan\"\n",
    "\n",
    "old_fn_no_offset = ['<s><s>You are the age of America, located in the States, California</s></s> Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim Crim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_fn_hf = ['<s>Mark Zuckerberg is the CEO of Facebook, located in San Alto, California.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>']\n",
    "\n",
    "no_ids_hf = ['<s>Mark Zuckerberg is the CEO of Facebook, located in San Alto, California.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>']\n",
    "\n",
    "arange_hf = ['<s></s><s>is the CEO of Facebook, located in San Francisco, California.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>. Facebook.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>']\n",
    "\n",
    "arange_no_offset = \"error\"\n",
    "\n",
    "old_fn_no_offset_hf = ['<s><s>Facebook is the CEO of Facebook, located in San Francisco, California.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 514, 50265])\n",
      "torch.Size([1, 514, 50265])\n"
     ]
    }
   ],
   "source": [
    "print(lv_result_logits.shape)\n",
    "print(hf_result_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>Explaining metaphysics to the nation. I wish he would explain his explanation. You, Bob, are rather insolent, you know, At being disappointed in your wish To supersede all warblers here below, And be the only blackbird in the dish. And then you overstrain yourself, or so And tumble downward like the flying fish Gasping on deck, because you soar too high, Bob, And fall for lack of moisture quite a dry Bob. And Wordsworth in a rather long Excursion (I think the quarto holds five hundred pages) Has given a sample from the vasty version Of his new system to perplex the sages. ’Tis poetry, at least by his assertion, And may appear so when the Dog Star rages, And he who understands it would be able To add a story to the tower of Babel. You gentlemen, by dint of long seclusion From better company, have kept your own At Keswick, and through still continued fusion Of one another’s minds at last have grown To deem, as a most logical conclusion, That poesy has wreaths for you alone. There is a narrowness in such a notion, Which makes me wish you’d change your lakes for ocean. I would not imitate the petty thought, Nor coin my self-love to so base a vice,  For all the glory your conversion brought,    Since gold alone should not have been its price. You have your salary; was’t for that you wrought?    And Wordsworth has his place in the Excise. You’re shabby fellows—true—but poets still And duly seated on the immortal hill. Your bays may hide the baldness of your brows,    Perhaps some virtuous blushes; let them go. To you I envy neither fruit nor boughs,  And for the fame you would engross below, The field is universal and allows    Scope to all such as feel the inherent glow. Scott, Rogers, Campbell, Moore, and Crabbe will try ’Gainst you the question with posterity. For me, who, wandering with pedestrian Muses,    Contend not with you on the winged’ steed, I wish your fate may yield ye, when she chooses,    The fame you envy and the skill you need. And recollect a poet nothing loses    In giving to his    </s>']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(lv_prompt[\"input_ids\"].array, skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>Explaining metaphysics to the nation. I wish he would explain his explanation. You, Bob, are rather insolent, you know, At being disappointed in your wish To supersede all warblers here below, And be the only blackbird in the dish. And then you overstrain yourself, or so And tumble downward like the flying fish Gasping on deck, because you soar too high, Bob, And fall for lack of moisture quite a dry Bob. And Wordsworth in a rather long Excursion (I think the quarto holds five hundred pages) Has given a sample from the vasty version Of his new system to perplex the sages. ’Tis poetry, at least by his assertion, And may appear so when the Dog Star rages, And he who understands it would be able To add a story to the tower of Babel. You gentlemen, by dint of long seclusion From better company, have kept your own At Keswick, and through still continued fusion Of one another’s minds at last have grown To deem, as a most logical conclusion, That poesy has wreaths for you alone. There is a narrowness in such a notion, Which makes me wish you’d change your lakes for ocean. I would not imitate the petty thought, Nor coin my self-love to so base a vice,  For all the glory your conversion brought,    Since gold alone should not have been its price. You have your salary; was’t for that you wrought?    And Wordsworth has his place in the Excise. You’re shabby fellows—true—but poets still And duly seated on the immortal hill. Your bays may hide the baldness of your brows,    Perhaps some virtuous blushes; let them go. To you I envy neither fruit nor boughs,  And for the fame you would engross below, The field is universal and allows    Scope to all such as feel the inherent glow. Scott, Rogers, Campbell, Moore, and Crabbe will try ’Gainst you the question with posterity. For me, who, wandering with pedestrian Muses,    Contend not with you on the winged’ steed, I wish your fate may yield ye, when she chooses,    The fame you envy and the skill you need. And recollect a poet nothing loses    In giving to his    </s>']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(prompt[\"input_ids\"], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s></s><s>Ph presented to the nation. I wish he would explain his explanation. You, Bob, are rather insolent, you know, At being disappointed in your wish To supersede all warblers here below, And be the only blackbird in the dish. And then you overstrain yourself, or so And tumble downward like the flying fish Gasping on deck, because you soar too high, Bob, And fall for lack of moisture Quite a dry Bob. And Wordsworth in a rather long Excursion (I think the quarto holds five hundred pages) Has given a sample from the fleshy version Of his new system to perplex the sages. ’Tis poetry, at least by his estimation, And may appear so when the Dog Star rages, And he who understands it would be able To add a story to the tower of Babel. You gentlemen, by dint of long seclusion From better company, have kept your own At Keswick, and through still continued fusion Of one another’s minds at last have grown To deem, as a most logical conclusion, That poetry has wreaths for you alone. There is a narrowness in such a notion, Which makes me wish you’d change your mind for me. I would not imitate the petty thought, Nor coin my self-love to so petty a vice,  For all the glory your conversion brought,    Since gold alone should not have been its price. You have your salary; wasn’t for that you virtuous?    And Wordsworth has his place in the Excise. You’re shabby fellows—true—but poets still And duly seated on the immortal hill. Your bays may hide the baldness of your brows,    Perhaps some virtuous blushes; let them go. To you I envy neither fruit nor boughs,  And for the fame you would engross below, The field is universal and allows    It to all such as feel the inherent glow. Scott, Rogers, Campbell, Moore, and Crabbe will try ’Gain to you the question with posterity. For me, who, wandering with the Muses,    Contend not with you on the winged’ steed, I wish your fate may yield ye, when she chooses,    The fame you envy and the skill you need. And recollect a poet nothing loses    In giving to his    </s>']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(lv_result_logits.argmax(dim=-1), skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s></s><s>Ph presented to the nation. I wish he would explain his explanation. You, Bob, are rather insolent, you know, At being disappointed in your wish To supersede all warblers here below, And be the only blackbird in the dish. And then you overstrain yourself, or so And tumble downward like the flying fish Gasping on deck, because you soar too high, Bob, And fall for lack of moisture Quite a dry Bob. And Wordsworth in a rather long Excursion (I think the quarto holds five hundred pages) Has given a sample from the fleshy version Of his new system to perplex the sages. ’Tis poetry, at least by his estimation, And may appear so when the Dog Star rages, And he who understands it would be able To add a story to the tower of Babel. You gentlemen, by dint of long seclusion From better company, have kept your own At Keswick, and through still continued fusion Of one another’s minds at last have grown To deem, as a most logical conclusion, That poetry has wreaths for you alone. There is a narrowness in such a notion, Which makes me wish you’d change your mind for me. I would not imitate the petty thought, Nor coin my self-love to so petty a vice,  For all the glory your conversion brought,    Since gold alone should not have been its price. You have your salary; wasn’t for that you virtuous?    And Wordsworth has his place in the Excise. You’re shabby fellows—true—but poets still And duly seated on the immortal hill. Your bays may hide the baldness of your brows,    Perhaps some virtuous blushes; let them go. To you I envy neither fruit nor boughs,  And for the fame you would engross below, The field is universal and allows    It to all such as feel the inherent glow. Scott, Rogers, Campbell, Moore, and Crabbe will try ’Gain to you the question with posterity. For me, who, wandering with the Muses,    Contend not with you on the winged’ steed, I wish your fate may yield ye, when she chooses,    The fame you envy and the skill you need. And recollect a poet nothing loses    In giving to his    </s>']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(hf_result_logits.argmax(dim=-1), skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "# unmasker = pipeline('fill-mask', model='roberta-base')\n",
    "# unmasker(\"The man worked as a <mask>.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[32.1017, -4.9437, 14.0598,  ..., -1.6564,  1.5099, 10.0954],\n",
       "         [ 5.6817, -4.4572, 22.0748,  ..., -4.5277, -5.5077,  5.5239],\n",
       "         [31.3908, -4.9682, 14.1496,  ..., -1.7012,  1.4606,  9.9348],\n",
       "         ...,\n",
       "         [ 3.6521, -3.7570, 17.7734,  ..., -0.4965,  0.1471,  5.7731],\n",
       "         [ 2.3458, -4.1200, 17.9135,  ..., -3.8652, -2.7885,  4.4862],\n",
       "         [10.6070, -4.2919, 23.0414,  ..., -2.8666, -5.3789,  6.4353]]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lv_result_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[32.1016, -4.9437, 14.0598,  ..., -1.6564,  1.5099, 10.0954],\n",
       "         [ 5.6817, -4.4572, 22.0748,  ..., -4.5277, -5.5077,  5.5239],\n",
       "         [31.3909, -4.9682, 14.1496,  ..., -1.7012,  1.4606,  9.9348],\n",
       "         ...,\n",
       "         [ 3.6521, -3.7570, 17.7734,  ..., -0.4965,  0.1471,  5.7731],\n",
       "         [ 2.3459, -4.1200, 17.9135,  ..., -3.8652, -2.7885,  4.4862],\n",
       "         [10.6070, -4.2919, 23.0414,  ..., -2.8666, -5.3789,  6.4353]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_result_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NamedArray(array=Array([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "         13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "         26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "         39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "         52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "         65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "         91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "        104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "        117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "        130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "        143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "        169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "        195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "        208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "        221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "        247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
       "        260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
       "        273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
       "        286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
       "        299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
       "        312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
       "        325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
       "        338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
       "        351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
       "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
       "        377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
       "        390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402,\n",
       "        403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415,\n",
       "        416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428,\n",
       "        429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441,\n",
       "        442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454,\n",
       "        455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
       "        468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480,\n",
       "        481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493,\n",
       "        494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506,\n",
       "        507, 508, 509, 510, 511, 512, 513]], dtype=int32), axes=(Axis(name='batch', size=1), Axis(name='position', size=514)))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     2,     0, 17297,  2633,     7,     5,  1226,     4,    38,\n",
       "          2813,    37,    74,  3922,    39,  8257,     4,   370,     6,  3045,\n",
       "             6,    32,  1195, 23799,  1342,     6,    47,   216,     6,   497,\n",
       "           145,  5779,    11,   110,  2813,   598, 31716, 12820,    70,   997,\n",
       "         25274,   259,   874,     6,   178,    28,     5,   129,   909, 15886,\n",
       "            11,     5,  8847,     4,   178,   172,    47,    81,  6031,  1851,\n",
       "          2512,     6,    50,    98,   178, 26566, 14659,   101,     5,  4731,\n",
       "          3539,   272,  9331,   154,    15,  9124,     6,   142,    47, 27283,\n",
       "           350,   239,     6,  3045,     6,   178,  1136,    13,  1762,     9,\n",
       "         16227, 27800,    10,  3841,  3045,     4,   178, 15690, 15142,    11,\n",
       "            10,  1195,   251, 22847, 29471,    36,   100,   206,     5, 18284,\n",
       "           139,  3106,   292,  6317,  6052,    43,  6233,   576,    10,  7728,\n",
       "            31,     5, 18940,   219,  1732,  1525,    39,    92,   467,     7,\n",
       "         33708,     5,   579,  3443,     4,    44,    27,   565,   354, 14665,\n",
       "             6,    23,   513,    30,    39, 32809,     6,   178,   189,  2082,\n",
       "            98,    77,     5,  8563,  2141,   910,  3443,     6,   178,    37,\n",
       "            54,  8832,    24,    74,    28,   441,   598,  1606,    10,   527,\n",
       "             7,     5,  9368,     9, 46060,     4,   370, 22629,     6,    30,\n",
       "           385,  2544,     9,   251,   842, 27953,  1740,   357,   138,     6,\n",
       "            33,  1682,   110,   308,   497, 18987,  8030,     6,     8,   149,\n",
       "           202,  1143, 24904,  1525,    65,   277,    17,    27,    29,  7283,\n",
       "            23,    94,    33,  3831,   598, 27907,     6,    25,    10,   144,\n",
       "         16437,  6427,     6,   280,  4202, 21262,    34,   885, 12353, 15354,\n",
       "            13,    47,  1937,     4,   345,    16,    10, 34139, 19649,  3361,\n",
       "            11,   215,    10,  9976,     6,  6834,   817,   162,  2813,    47,\n",
       "            17,    27,   417,   464,   110,  1508,    13,   162,     4,    38,\n",
       "            74,    45, 41379,     5, 25070,   802,     6,  6567, 12911,   127,\n",
       "          1403,    12, 17693,     7,    98, 25070,    10,  2626,     6,  1437,\n",
       "           286,    70,     5, 12594,   110, 10012,  1146,     6,  1437,  1437,\n",
       "          1437,  1773,  1637,  1937,   197,    45,    33,    57,    63,   425,\n",
       "             4,   370,    33,   110,  5391,   131,   938,    17,    27,    90,\n",
       "            13,    14,    47, 41890,   116,  1437,  1437,  1437,   178, 15690,\n",
       "         15142,    34,    39,   317,    11,     5, 22847,  1496,     4,   370,\n",
       "            17,    27,   241,  1481, 35878, 36304,   578, 29225,   578,  4297,\n",
       "         33297,   202,   178, 27069, 17630,    15,     5, 33427,  9910,     4,\n",
       "          2486,   741,  4113,   189,  7433,     5, 24876,  1825,     9,   110,\n",
       "         27423,    29,     6,  1437,  1437,  1437,  6259,   103, 41890,  3089,\n",
       "         27843,   131,   905,   106,   213,     4,   598,    47,    38, 29778,\n",
       "          5063,  6231,  3486,   741,  4894,    29,     6,  1437,   178,    13,\n",
       "             5,  9444,    47,    74, 20407, 14500,   874,     6,    20,   882,\n",
       "            16, 10547,     8,  2386,  1437,  1437,  1437,    85,     7,    70,\n",
       "           215,    25,   619,     5, 17886, 20059,     4,  1699,     6,  7541,\n",
       "             6,  5925,     6,  3404,     6,     8, 25908,  1610,    40,   860,\n",
       "            44,    27,   534,  1851,     7,    47,     5,   864,    19, 11566,\n",
       "          1571,     4,   286,   162,     6,    54,     6, 26884,    19,     5,\n",
       "          5950,   293,     6,  1437,  1437,  1437,  7555,  1397,    45,    19,\n",
       "            47,    15,     5,  5897,   196,    17,    27,  1690, 10247,     6,\n",
       "            38,  2813,   110,  7658,   189,  3363, 32440,     6,    77,    79,\n",
       "         19662,     6,  1437,  1437,  1437,    20,  9444,    47, 29778,     8,\n",
       "             5,  6707,    47,   240,     4,   178, 37941, 17601,    10, 16893,\n",
       "          1085, 13585,  1437,  1437,  1437,    96,  1311,     7,    39,  1437,\n",
       "          1437,  1437,  1437,     2]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lv_result_logits.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     2,     0, 17297,  2633,     7,     5,  1226,     4,    38,\n",
       "          2813,    37,    74,  3922,    39,  8257,     4,   370,     6,  3045,\n",
       "             6,    32,  1195, 23799,  1342,     6,    47,   216,     6,   497,\n",
       "           145,  5779,    11,   110,  2813,   598, 31716, 12820,    70,   997,\n",
       "         25274,   259,   874,     6,   178,    28,     5,   129,   909, 15886,\n",
       "            11,     5,  8847,     4,   178,   172,    47,    81,  6031,  1851,\n",
       "          2512,     6,    50,    98,   178, 26566, 14659,   101,     5,  4731,\n",
       "          3539,   272,  9331,   154,    15,  9124,     6,   142,    47, 27283,\n",
       "           350,   239,     6,  3045,     6,   178,  1136,    13,  1762,     9,\n",
       "         16227, 27800,    10,  3841,  3045,     4,   178, 15690, 15142,    11,\n",
       "            10,  1195,   251, 22847, 29471,    36,   100,   206,     5, 18284,\n",
       "           139,  3106,   292,  6317,  6052,    43,  6233,   576,    10,  7728,\n",
       "            31,     5, 18940,   219,  1732,  1525,    39,    92,   467,     7,\n",
       "         33708,     5,   579,  3443,     4,    44,    27,   565,   354, 14665,\n",
       "             6,    23,   513,    30,    39, 32809,     6,   178,   189,  2082,\n",
       "            98,    77,     5,  8563,  2141,   910,  3443,     6,   178,    37,\n",
       "            54,  8832,    24,    74,    28,   441,   598,  1606,    10,   527,\n",
       "             7,     5,  9368,     9, 46060,     4,   370, 22629,     6,    30,\n",
       "           385,  2544,     9,   251,   842, 27953,  1740,   357,   138,     6,\n",
       "            33,  1682,   110,   308,   497, 18987,  8030,     6,     8,   149,\n",
       "           202,  1143, 24904,  1525,    65,   277,    17,    27,    29,  7283,\n",
       "            23,    94,    33,  3831,   598, 27907,     6,    25,    10,   144,\n",
       "         16437,  6427,     6,   280,  4202, 21262,    34,   885, 12353, 15354,\n",
       "            13,    47,  1937,     4,   345,    16,    10, 34139, 19649,  3361,\n",
       "            11,   215,    10,  9976,     6,  6834,   817,   162,  2813,    47,\n",
       "            17,    27,   417,   464,   110,  1508,    13,   162,     4,    38,\n",
       "            74,    45, 41379,     5, 25070,   802,     6,  6567, 12911,   127,\n",
       "          1403,    12, 17693,     7,    98, 25070,    10,  2626,     6,  1437,\n",
       "           286,    70,     5, 12594,   110, 10012,  1146,     6,  1437,  1437,\n",
       "          1437,  1773,  1637,  1937,   197,    45,    33,    57,    63,   425,\n",
       "             4,   370,    33,   110,  5391,   131,   938,    17,    27,    90,\n",
       "            13,    14,    47, 41890,   116,  1437,  1437,  1437,   178, 15690,\n",
       "         15142,    34,    39,   317,    11,     5, 22847,  1496,     4,   370,\n",
       "            17,    27,   241,  1481, 35878, 36304,   578, 29225,   578,  4297,\n",
       "         33297,   202,   178, 27069, 17630,    15,     5, 33427,  9910,     4,\n",
       "          2486,   741,  4113,   189,  7433,     5, 24876,  1825,     9,   110,\n",
       "         27423,    29,     6,  1437,  1437,  1437,  6259,   103, 41890,  3089,\n",
       "         27843,   131,   905,   106,   213,     4,   598,    47,    38, 29778,\n",
       "          5063,  6231,  3486,   741,  4894,    29,     6,  1437,   178,    13,\n",
       "             5,  9444,    47,    74, 20407, 14500,   874,     6,    20,   882,\n",
       "            16, 10547,     8,  2386,  1437,  1437,  1437,    85,     7,    70,\n",
       "           215,    25,   619,     5, 17886, 20059,     4,  1699,     6,  7541,\n",
       "             6,  5925,     6,  3404,     6,     8, 25908,  1610,    40,   860,\n",
       "            44,    27,   534,  1851,     7,    47,     5,   864,    19, 11566,\n",
       "          1571,     4,   286,   162,     6,    54,     6, 26884,    19,     5,\n",
       "          5950,   293,     6,  1437,  1437,  1437,  7555,  1397,    45,    19,\n",
       "            47,    15,     5,  5897,   196,    17,    27,  1690, 10247,     6,\n",
       "            38,  2813,   110,  7658,   189,  3363, 32440,     6,    77,    79,\n",
       "         19662,     6,  1437,  1437,  1437,    20,  9444,    47, 29778,     8,\n",
       "             5,  6707,    47,   240,     4,   178, 37941, 17601,    10, 16893,\n",
       "          1085, 13585,  1437,  1437,  1437,    96,  1311,     7,    39,  1437,\n",
       "          1437,  1437,  1437,     2]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_result_logits.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0, 43043,  8173, 41724, 33823,     7,     5,  1226,     4,\n",
       "           38,  2813,    37,    74,  3922,    39,  8257,     4,   370,\n",
       "            6,  3045,     6,    32,  1195, 23799,  1342,     6,    47,\n",
       "          216,     6,   497,   145,  5779,    11,   110,  2813,   598,\n",
       "        31716, 12820,    70,   997, 25274,   259,   874,     6,   178,\n",
       "           28,     5,   129,   909, 15886,    11,     5,  8847,     4,\n",
       "          178,   172,    47,    81,  6031,  1851,  2512,     6,    50,\n",
       "           98,   178, 26566, 14659,   101,     5,  4731,  3539,   272,\n",
       "         9331,   154,    15,  9124,     6,   142,    47, 27283,   350,\n",
       "          239,     6,  3045,     6,   178,  1136,    13,  1762,     9,\n",
       "        16227,  1341,    10,  3841,  3045,     4,   178, 15690, 15142,\n",
       "           11,    10,  1195,   251, 22847, 29471,    36,   100,   206,\n",
       "            5, 18284,   139,  3106,   292,  6317,  6052,    43,  6233,\n",
       "          576,    10,  7728,    31,     5,  4714,   219,  1732,  1525,\n",
       "           39,    92,   467,     7, 33708,     5,   579,  3443,     4,\n",
       "           44,    27,   565,   354, 14665,     6,    23,   513,    30,\n",
       "           39, 19395,     6,   178,   189,  2082,    98,    77,     5,\n",
       "         8563,  2141,   910,  3443,     6,   178,    37,    54,  8832,\n",
       "           24,    74,    28,   441,   598,  1606,    10,   527,     7,\n",
       "            5,  9368,     9, 46060,     4,   370, 22629,     6,    30,\n",
       "          385,  2544,     9,   251,   842, 27953,  1740,   357,   138,\n",
       "            6,    33,  1682,   110,   308,   497, 18987,  8030,     6,\n",
       "            8,   149,   202,  1143, 24904,  1525,    65,   277,    17,\n",
       "           27,    29,  7283,    23,    94,    33,  3831,   598, 27907,\n",
       "            6,    25,    10,   144, 16437,  6427,     6,   280,  4202,\n",
       "        43659,    34,   885, 12353, 15354,    13,    47,  1937,     4,\n",
       "          345,    16,    10, 34139, 19649,  3361,    11,   215,    10,\n",
       "         9976,     6,  6834,   817,   162,  2813,    47,    17,    27,\n",
       "          417,   464,   110, 17529,    13,  6444,     4,    38,    74,\n",
       "           45, 41379,     5, 25070,   802,     6,  6567, 12911,   127,\n",
       "         1403,    12, 17693,     7,    98,  1542,    10,  2626,     6,\n",
       "         1437,   286,    70,     5, 12594,   110, 10012,  1146,     6,\n",
       "         1437,  1437,  1437,  1773,  1637,  1937,   197,    45,    33,\n",
       "           57,    63,   425,     4,   370,    33,   110,  5391,   131,\n",
       "           21,    17,    27,    90,    13,    14,    47, 37058,   116,\n",
       "         1437,  1437,  1437,   178, 15690, 15142,    34,    39,   317,\n",
       "           11,     5, 22847,  1496,     4,   370,    17,    27,   241,\n",
       "         1481, 35878, 36304,   578, 29225,   578,  4297, 33297,   202,\n",
       "          178, 27069, 17630,    15,     5, 33427,  9910,     4,  2486,\n",
       "          741,  4113,   189,  7433,     5, 24876,  1825,     9,   110,\n",
       "        27423,    29,     6,  1437,  1437,  1437,  6259,   103, 41890,\n",
       "         3089, 27843,   131,   905,   106,   213,     4,   598,    47,\n",
       "           38, 29778,  5063,  6231,  3486,   741,  4894,    29,     6,\n",
       "         1437,   178,    13,     5,  9444,    47,    74, 20407, 14500,\n",
       "          874,     6,    20,   882,    16, 10547,     8,  2386,  1437,\n",
       "         1437,  1437, 30108,     7,    70,   215,    25,   619,     5,\n",
       "        17886, 20059,     4,  1699,     6,  7541,     6,  5925,     6,\n",
       "         3404,     6,     8, 25908,  1610,    40,   860,    44,    27,\n",
       "          534,  1851,   620,    47,     5,   864,    19, 11566,  1571,\n",
       "            4,   286,   162,     6,    54,     6, 26884,    19, 11038,\n",
       "         5950,   293,     6,  1437,  1437,  1437,  7555,  1397,    45,\n",
       "           19,    47,    15,     5,  5897,   196,    17,    27,  1690,\n",
       "        10247,     6,    38,  2813,   110,  7658,   189,  3363, 32440,\n",
       "            6,    77,    79, 19662,     6,  1437,  1437,  1437,    20,\n",
       "         9444,    47, 29778,     8,     5,  6707,    47,   240,     4,\n",
       "          178, 37941, 17601,    10, 16893,  1085, 13585,  1437,  1437,\n",
       "         1437,    96,  1311,     7,    39,  1437,  1437,  1437,  1437,\n",
       "            2]], dtype=int64)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(prompt[\"input_ids\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "levanter2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
