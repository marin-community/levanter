data:
  cache_dir: "- gs://levanter-data/tokenized/markweb_llama/"
  tokenizer: "meta-llama/Llama-2-7b-hf"
  configs:
    openwebtext:
      train_urls:
        - "- gs://pubmed-mosaic/openwebtext-sharded/openwebtext_train.{1..128}-of-128.jsonl.gz"
      validation_urls:
        - "- gs://pubmed-mosaic/openwebtext-sharded/openwebtext_val.{1..8}-of-8.jsonl.gz"
    # these are just for eval
    "paloma/4chan":
      validation_urls:
        - - gs://levanter-data/paloma/4chan_meta_sep/val/val*.jsonl.gz
    "paloma/c4_100_domains":
      validation_urls:
        - - gs://levanter-data/paloma/c4_100_domains/val/val*.jsonl.gz
    "paloma/c4_en":
      validation_urls:
        - - gs://levanter-data/paloma/c4_en/val/val*.jsonl.gz
    "paloma/dolma-v1_5":
      validation_urls:
        - - gs://levanter-data/paloma/dolma-v1_5/val/val*.jsonl.gz
    "paloma/dolma_100_programing_languages":
      validation_urls:
        - - gs://levanter-data/paloma/dolma_100_programing_languages/val/val*.jsonl.gz
    "paloma/dolma_100_subreddits":
      validation_urls:
        - - gs://levanter-data/paloma/dolma_100_subreddits/val/val*.jsonl.gz
    "paloma/falcon-refinedweb":
      validation_urls:
        - - gs://levanter-data/paloma/falcon-refinedweb/val/val*.jsonl.gz
    "paloma/gab":
      validation_urls:
        - - gs://levanter-data/paloma/gab/val/val*.jsonl.gz
    "paloma/m2d2_s2orc_unsplit":
      validation_urls:
        - - gs://levanter-data/paloma/m2d2_s2orc_unsplit/val/val*.jsonl.gz
    "paloma/m2d2_wikipedia_unsplit":
      validation_urls:
        - - gs://levanter-data/paloma/m2d2_wikipedia_unsplit/val/val*.jsonl.gz
    "paloma/manosphere_meta_sep":
      validation_urls:
        - - gs://levanter-data/paloma/manosphere_meta_sep/val/val*.jsonl.gz
    "paloma/mc4":
      validation_urls:
        - - gs://levanter-data/paloma/mc4/val/val*.jsonl.gz
    "paloma/ptb":
      validation_urls:
        - - gs://levanter-data/paloma/ptb/val/val*.jsonl.gz
    "paloma/redpajama":
      validation_urls:
        - - gs://levanter-data/paloma/redpajama/val/val*.jsonl.gz
    "paloma/twitterAAE_HELM_fixed":
      validation_urls:
        - - gs://levanter-data/paloma/twitterAAE_HELM_fixed/val/val*.jsonl.gz
    "paloma/wikitext_103":
      validation_urls:
        - - gs://levanter-data/paloma/wikitext_103/val/val*.jsonl.gz

  train_weights:
    openwebtext: 1.0
    paloma/4chan: 0.0
    paloma/c4_100_domains: 0.0
    paloma/c4_en: 0.0
    paloma/dolma-v1_5: 0.0
    paloma/dolma_100_programing_languages: 0.0
    paloma/dolma_100_subreddits: 0.0
    paloma/falcon-refinedweb: 0.0
    paloma/gab: 0.0
    paloma/m2d2_s2orc_unsplit: 0.0
    paloma/m2d2_wikipedia_unsplit: 0.0
    paloma/manosphere_meta_sep: 0.0
    paloma/mc4: 0.0
    paloma/ptb: 0.0
    paloma/redpajama: 0.0
    paloma/twitterAAE_HELM_fixed: 0.0
    paloma/wikitext_103: 0.0
model:
  # 1B class model
  type: llama
  seq_len: 2048
  hidden_dim: 2048
  intermediate_dim: 4096
  num_layers: 24
  num_heads: 32
  num_kv_heads: 32
  use_flash_attention: True
  flash_attention_block_size: 2048
trainer:
  tracker:
    type: wandb
    project: "markweb"
    tags: ["owt", "llama", "web_comparison"]

  mp: p=f32,c=bfloat16
  train_batch_size: 512
  num_train_steps: 50000
  steps_per_eval: 1000
  per_device_eval_parallelism: 64
  tensor_parallel_axes: ["mlp", "heads"]
  fsdp_axis: "embed"
  batch_axis: "batch"
optimizer:
  learning_rate: 2E-4
  weight_decay: 0.1
  min_lr_ratio: 0.1
