# Levanter 405B Model Forward Pass on v4-256 TPU

## Problem Statement
- Loading Llama-3.1-405B model on v4-256 TPU (128 chips √ó 32GB HBM = 4096 GiB total)
- Model requires ~1620 GiB in fp32, should fit but getting OOM errors
- Error: `RESOURCE_EXHAUSTED: Allocation (size=135291469824) would exceed memory (size=34359738368)`
- Current config uses tensor parallelism on `["embed", "mlp", "heads"]` but model axis not properly utilized

## Hardware Specifications
- **v4-256**: 128 TPU chips, each with 32 GiB HBM memory
- **Total Memory**: 4096 GiB 
- **Per-chip Memory**: 32 GiB
- **Model Size**: 405B parameters ‚âà 1620 GiB in fp32

## Current Configuration Analysis
File: `/Users/ahmed/code/levanter/config/books/eval_careless_llama3.1_405b_hp1.yaml`

### Current Settings:
```yaml
trainer:
  tensor_parallel_axes: ["mlp", "heads"]
  fsdp_axis: "embed" 
  batch_axis: "batch"
  per_device_eval_parallelism: -1
eval_batch_size: 1
```

### Model Architecture:
- **Layers**: 126
- **Hidden Dim**: 16384
- **Intermediate Dim**: 53248  
- **Num Heads**: 128
- **Num KV Heads**: 8

## Relevant Documentation References

### Core Partitioning Documentation
1. **Main Partitioning Guide**: `/Users/ahmed/code/levanter/haliax/docs/partitioning.md`
   - Lines 7-10: Tutorial link to [Distributed Training in Haliax Colab](https://colab.research.google.com/drive/1QX4yH3zRFF3Xiibf1aahETcSQ5nbcUMz)
   - Lines 60-85: Core Haliax partitioning concepts with axis mappings

2. **Trainer Implementation**: `/Users/ahmed/code/levanter/src/levanter/trainer.py`
   - Lines 669-676: `tensor_parallel_axes` configuration 
   - Lines 843-847: Tensor parallel axes mapping to `ResourceAxis.MODEL`
   - Lines 791-798: Device mesh creation with `create_fsdp_mesh()`
   - Lines 839-867: `compute_axis_mapping` and `parameter_axis_mapping` properties

3. **ResourceAxis Enum**: `/Users/ahmed/code/levanter/haliax/src/haliax/partitioning.py`
   - Lines 53-59: `ResourceAxis.MODEL/DATA/REPLICA` definitions

### Key Configuration Parameters
From `TrainerConfig` class (trainer.py:637-960):

```python
# Partitioning configuration
tensor_parallel_axes: Optional[List[str]] = None  # line 669
fsdp_axis: Optional[Union[str, List[str]]] = "embed"  # line 668
model_axis_size: int = 1  # line 680
replica_ici_axis_size: int = 1  # line 679
replica_dcn_axis_size: int = 1  # line 682
```

### Device Mesh Configuration
- Lines 791-798: `create_fsdp_mesh()` creates mesh with ICI/DCN axis sizing
- Lines 815-831: Automatic calculation of data axis sizes based on available devices

## Key Insights from Distributed Training Tutorial

**Critical Learning from scaling_transformers_haliax.ipynb**:

### FSDP vs Tensor Parallelism Strategy
- **FSDP**: Shards parameters across devices (`{"embed": "data"}`)
- **Data Parallel**: Shards batch across devices (`{"batch": "data"}`)  
- **Key Pattern**: Use **separate axis mappings** for compute vs parameters

### Proper Configuration Pattern
```python
# From notebook cells 29, 31
compute_axis_mapping = {"batch": "data"}  # For forward/backward pass
param_axis_mapping = {"embed": "data"}    # For parameter storage
```

### Essential Implementation Details
1. **Context Manager Usage**: 
   ```python
   with hax.axis_mapping(compute_axis_mapping):
       loss, grads = grad_loss(model, input_ids)
   ```
2. **Model Initialization**: Must shard model with `param_axis_mapping`
3. **Named JIT**: Use `@hax.named_jit` for distributed functions

## Tensor Parallelism Insights from Tutorial

**Key Tutorial Insights from `/Users/ahmed/code/levanter/tensor_parallel.ipynb`**:

### Core Tensor Parallelism Pattern
```python
# Create 2D mesh with model parallelism
model_parallelism = 2  # Use 8 for 405B
mp_mesh = Mesh(onp.array(jax.devices()).reshape(-1, model_parallelism), ("data", "model"))

# Define axis mappings
tp_axis_mapping = {"mlp": "model", "head": "model"}
compute_axis_mapping = {**dp_axis_mapping, **tp_axis_mapping}
param_axis_mapping = {**fsdp_axis_mapping, **tp_axis_mapping}
```

### Key Differences from Current Approach
1. **2D Device Mesh**: Tutorial uses `("data", "model")` mesh, not just data parallelism
2. **Pure Tensor Parallelism**: Maps both "mlp" and "head" to "model" axis
3. **Separate Mappings**: Uses distinct `compute_axis_mapping` and `param_axis_mapping`
4. **No FSDP Needed**: User wants pure tensor parallelism, not FSDP combination

### Tensor Parallelism for 405B Model
- **MLP Parallelism**: Distributes 53248 intermediate dim across 8 devices ‚Üí ~6656 per device
- **Attention Parallelism**: Distributes 128 heads across 8 devices ‚Üí 16 heads per device
- **Memory Reduction**: 135GB attention matrices distributed across 8 devices ‚Üí ~17GB per device
- **Target**: Fit within 32GB per chip on v4-128

## Solution Attempts Log

### Attempt 1: Current Configuration Analysis
**Problem Identified**: 
- Error shows allocation of 135GB (126√ó16384√ó16384√ó4 bytes) exceeding 32GB per-chip limit
- Current config mixes FSDP with tensor parallelism incorrectly
- User wants **pure tensor parallelism**, not FSDP+TP combination

### Attempt 2: Apply Pure Tensor Parallelism (Based on Tutorial)
**Strategy**: Implement pure tensor parallelism using tutorial's 2D mesh pattern
**Required Changes**:
1. ‚úÖ Use 2D device mesh with `model_axis_size: 16` for v4-256
2. üîÑ Remove `fsdp_axis` - user doesn't want FSDP
3. ‚úÖ Keep `tensor_parallel_axes: ["embed", "mlp", "heads"]` for pure TP
4. ‚úÖ Update run name to reflect v4-256 tensor parallelism approach

**Expected Behavior**:
- MLP computation distributed across 16 model-parallel devices
- Attention heads distributed across 16 model-parallel devices  
- 135GB attention matrices ‚Üí ~8.4GB per device
- Should fit comfortably within 32GB per chip on v4-256

**Implementation**: ‚úÖ Modified YAML config and ‚úÖ Fixed eval script model loading

### Critical Fix Applied: Model Loading Bug
**Problem Found**: In `eval_careless_lm.py:303`, the model loading was missing tensor parallelism:
```python
# BEFORE (causes OOM):
model = converter.load_pretrained(cfg.model.model_type, ref=hf_ref, dtype=mp.compute_dtype)

# AFTER (with tensor parallelism):
model = converter.load_pretrained(cfg.model.model_type, ref=hf_ref, dtype=mp.compute_dtype, axis_mapping=pmapping)
```

**Root Cause**: The `load_pretrained()` function was defaulting to `axis_mapping=None`, which meant the 405B model was loaded without any tensor parallelism sharding, causing the entire model to try to fit on single devices.

### Attempt 3: v4-256 Debugging - Best Effort Sharding Analysis
**Hardware Upgrade**: Moved from v4-128 (64 chips) to v4-256 (128 chips)

**Problem Found in best_effort.log**:
- ‚ùå Mesh configuration: `mesh.devices.shape=(1, 64, 2)` - only 2 devices on model axis!
- ‚ùå All tensors sharded on 'data' axis only: `PartitionSpec(None, 'data')`
- ‚ùå Model axis ignored: Large tensors like `[16384, 16384]` not distributed across model axis
- ‚ùå `model_axis_size: 2` too small for 405B model

**Debug Output Analysis**:
```
[DEBUG] best_effort_sharding: shape=[16384, 16384], mesh.axis_names=('replica', 'data', 'model')
[DEBUG] mesh.devices.shape=(1, 64, 2), fsdp_axis=1, num_devices=64
[DEBUG] Will shard axis 1 (size=16384) across 64 DATA devices
[DEBUG] Final sharding: PartitionSpec(None, 'data')
```

**Critical Issues Identified**:
1. **Insufficient Model Parallelism**: `model_axis_size: 2` ‚Üí should be 16 for 405B
2. **Ignored Tensor Parallel Axes**: Best effort sharding falling back to FSDP-style data sharding
3. **Memory Bottleneck**: 135GB attention matrices not distributed across model axis

**Solution Applied**:
1. ‚úÖ Increased `model_axis_size: 2` ‚Üí `model_axis_size: 16`
2. ‚úÖ Updated run name to reflect v4-256 and model16 configuration
3. ‚úÖ Updated output paths for v4-256 experiment tracking

## Memory Calculation
- **Attention Matrix**: 126 layers √ó 16384¬≤ √ó 4 bytes (fp32) = ~135 GB per device
- **v4-256 Target**: Distribute across 16 devices: 135GB √∑ 16 = ~8.4GB per device
- **Solution**: Use `model_axis_size: 16` on v4-256 for proper memory distribution

## Final Working Configuration (v4-256)

### Key Settings for 405B Success:
```yaml
trainer:
  tensor_parallel_axes: ["mlp", "heads"]  # Pure TP: removed "embed" 
  model_axis_size: 16  # Critical: 16 devices for model parallelism
  batch_axis: "batch"
  # No fsdp_axis - pure tensor parallelism approach
```

### Expected Device Mesh:
- **Total devices**: 128 (v4-256)
- **Model axis**: 16 devices (for 405B parameter distribution)
- **Data axis**: 8 devices (128 √∑ 16 = 8)
- **Mesh shape**: `(1, 8, 16)` ‚Üí `('replica', 'data', 'model')`

### Memory Distribution:
- **135GB attention matrices** ‚Üí 8.4GB per device (135 √∑ 16)
- **405B parameters** distributed across 16 model-parallel devices
- **Fits comfortably** within 32GB per chip on v4-256

---

## ‚ùå PREVIOUS ATTEMPT FAILED - CORRECT ROOT CAUSE IDENTIFIED

### Analysis of new_tp.log Results 
**Mesh Configuration**: ‚úÖ CORRECT 
- Mesh shape: `(1, 8, 16)` ‚Üí `('replica', 'data', 'model')`
- 16 devices available for MODEL axis (tensor parallelism)
- 8 devices available for DATA axis (batch parallelism)

**Critical Problem Found**: ‚ùå `best_effort_sharding` IGNORES MODEL AXIS
```
[DEBUG] mesh.devices.shape=(1, 8, 16), fsdp_axis=1, num_devices=8
[DEBUG] Will shard axis 1 (size=16384) across 8 DATA devices  
[DEBUG] Final sharding: PartitionSpec(None, 'data')
```

### TRUE ROOT CAUSE: `best_effort_sharding` Function is FSDP-Only ‚ùå

**Problem in `src/levanter/utils/jax_utils.py:304-305`**:
```python
# ONLY considers DATA axis, ignores MODEL axis completely!
fsdp_axis = mesh.axis_names.index(hax.partitioning.ResourceAxis.DATA)
num_devices = mesh.devices.shape[fsdp_axis]  # Gets 8, not 16!
```

**What's happening**:
1. **Large tensors**: `[16384, 16384]` (1GB each √ó 126 layers = 126GB+)
2. **Current sharding**: Across 8 DATA devices ‚Üí 126GB √∑ 8 = 15.8GB per device
3. **Still OOM**: 15.8GB + model weights + activations > 32GB per chip
4. **What we need**: Across 16 MODEL devices ‚Üí 126GB √∑ 16 = 7.9GB per device

### ‚úÖ CORRECT SOLUTION: Fix `best_effort_sharding` for Tensor Parallelism

**Problem**: `best_effort_sharding` was designed for FSDP, not tensor parallelism
**Fix needed**: Modify function to use MODEL axis when available and tensors are large

**Implementation Plan**:
```python
# In src/levanter/utils/jax_utils.py:302+
def best_effort_sharding(shape, *, devices=None, mesh=None):
    # ... existing code ...
    else:
        # Check if we have a model axis available for tensor parallelism
        model_axis_idx = None
        if hax.partitioning.ResourceAxis.MODEL in mesh.axis_names:
            model_axis_idx = mesh.axis_names.index(hax.partitioning.ResourceAxis.MODEL)
            model_devices = mesh.devices.shape[model_axis_idx]
        
        # Get DATA axis for fallback
        fsdp_axis = mesh.axis_names.index(hax.partitioning.ResourceAxis.DATA)
        data_devices = mesh.devices.shape[fsdp_axis]
        
        # Calculate tensor size in bytes (fp32)
        tensor_size_mb = np.prod(shape) * 4 / (1024 * 1024)
        
        # Use MODEL axis for large tensors when tensor parallelism is available
        if (model_axis_idx is not None and model_devices > 1 and 
            tensor_size_mb > 128):  # 128MB threshold
            
            # Try to shard on MODEL axis first
            for i in range(len(shape) - 1, -1, -1):
                if shape[i] % model_devices == 0:
                    axis_sharding = [None] * len(shape)
                    axis_sharding[i] = hax.partitioning.ResourceAxis.MODEL
                    return NamedSharding(mesh, PartitionSpec(*axis_sharding))
        
        # Fallback to DATA axis (original FSDP behavior)
        # ... existing DATA axis logic ...
```

**Key Changes**:
1. **Tensor size check**: 128MB threshold (covers 16384√ó16384√ó4 = 1GB matrices)
2. **MODEL axis priority**: For large tensors when `model_axis_size > 1`
3. **Backwards compatible**: Falls back to DATA axis for small tensors

## ‚úÖ SOLUTION IMPLEMENTED: Fixed `best_effort_sharding`

### Changes Made:
1. **Modified `src/levanter/utils/jax_utils.py`**: Added MODEL axis support to `best_effort_sharding`
2. **Updated config name**: `llama_3.1_405b_hp1_token_v4_256_model16_tp_best_effort_fix`

### Expected Behavior:
- **Large tensors** (>128MB): `PartitionSpec(None, 'model')` across 16 devices  
- **Small tensors** (<128MB): `PartitionSpec(None, 'data')` across 8 devices
- **16384√ó16384 matrices**: 1GB √∑ 16 = 64MB per device ‚úÖ
- **Total 405B model**: Should fit within 32GB √ó 128 = 4TB total memory

### Debug Output Will Show:
```
[DEBUG] Large tensor detected, trying MODEL axis with 16 devices
[DEBUG] Using MODEL axis: shard axis 1 (size=16384) across 16 MODEL devices  
[DEBUG] Final sharding: PartitionSpec(None, 'model')
```

**Next Step**: Test the updated configuration to verify MODEL axis is used for large tensors.

---

## ‚ùå SECOND FAILURE: Model Loading Memory Allocation 

### Analysis of best_effort_claude.log

**‚úÖ Sharding Fix WORKS Correctly**:
- Large tensors (1024MB): `PartitionSpec(None, 'model')` across 16 devices ‚úÖ
- Small tensors (64MB): `PartitionSpec(None, 'data')` across 8 devices ‚úÖ  
- Mesh configuration perfect: `(1, 8, 16)` ‚Üí `('replica', 'data', 'model')` ‚úÖ

**‚ùå New Problem: Model Loading Memory Allocation**:
```
Loading weights:  12%|‚ñà‚ñé        | 1/8 [00:03<00:26,  3.85s/it]
ValueError: RESOURCE_EXHAUSTED: Error allocating device buffer: 
Attempting to allocate 104.00M. That was not possible. 
There are 15.31M free.; (0x0x0_HBM0)
```

### Root Cause: Loading Process Memory Bottleneck

**Location**: `src/levanter/compat/hf_checkpoints.py:1130` in `_shard_best_effort`  
**Function**: `jax.make_array_from_callback` during model weight loading
**Problem**: Even with correct sharding decisions, the loading process requires temporary memory

**Progress**: 
- ‚úÖ **Memory request reduced**: 208MB ‚Üí 104MB (50% improvement from sharding fix)
- ‚ùå **Still insufficient**: Device has only 15.31MB free, needs 104MB
- ‚ùå **Early failure**: Only 12% of weights loaded before OOM

### Next Solution Options

#### Option 1: Loading Memory Optimization üîß
**Target**: `src/levanter/compat/hf_checkpoints.py` loading pipeline
**Changes needed**:
1. **Batch loading**: Load fewer weight shards simultaneously  
2. **Streaming loading**: Load and shard weights incrementally
3. **Memory cleanup**: Force garbage collection between weight loads

#### Option 2: Aggressive Model Sharding üìä
**Current threshold**: 128MB ‚Üí **New threshold**: 32MB or 16MB
**Rationale**: More tensors use MODEL axis (16 devices) vs DATA axis (8 devices)
**Implementation**: Lower `tensor_size_mb > 128` to `tensor_size_mb > 32`

#### Option 3: Sequential Loading üîÑ  
**Strategy**: Load model layer-by-layer instead of parallel loading
**Benefits**: Reduces peak memory during loading phase
**Trade-off**: Slower loading but more memory efficient

#### Option 4: CPU Fallback Loading üíæ
**Approach**: Load large weights on CPU first, then transfer to TPU devices
**Benefits**: Avoid TPU memory fragmentation during loading
**Implementation**: Modify `load_pretrained` to use CPU staging

## üöÄ NEXT ATTEMPT: Option 2 - Aggressive Model Sharding

**Quick Fix**: Lower threshold from 128MB to 32MB in `best_effort_sharding`

**Reasoning**:
- 64MB tensors currently use DATA axis (8 devices) ‚Üí 8MB per device
- With MODEL axis (16 devices) ‚Üí 4MB per device (50% less memory)
- More tensors distributed across MODEL axis = better memory distribution
- Fastest fix to implement and test

**Implementation Applied**:
1. ‚úÖ **Modified threshold**: `tensor_size_mb > 128` ‚Üí `tensor_size_mb > 32`
2. ‚úÖ **Updated config name**: `llama_3.1_405b_hp1_token_v4_256_model16_tp_aggressive_32mb`

**Expected Impact**:
- `shape=[1024, 16384]` = 64MB tensors ‚Üí MODEL axis instead of DATA axis
- More aggressive distribution of medium-sized tensors  
- Reduced memory pressure during loading phase
- Debug will show: `"Large tensor detected"` for 64MB+ tensors (vs previous 128MB+)

---

## ‚úÖ SIGNIFICANT PROGRESS: 32MB Threshold Success + Remaining Issue

### Analysis of 32mb.log Results

**‚úÖ MAJOR IMPROVEMENTS Achieved**:
1. **Model loading succeeded much further**: Multiple "Loading weights: 100%" completions 
2. **64MB tensors now use MODEL axis**: `shape=[1024, 16384]` ‚Üí `PartitionSpec(None, 'model')` ‚úÖ
3. **Memory allocation dramatically improved**: 
   - **Previous failure**: 15.31MB free, needed 104MB
   - **Current attempt**: 178.29MB free, needs 208MB  
   - **11x improvement in available memory**: 15MB ‚Üí 178MB

**‚ùå Still Failing but Much Later**:
```
Loading weights:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:05<00:05,  5.92s/it]
ValueError: RESOURCE_EXHAUSTED: Error allocating device buffer: 
Attempting to allocate 208.00M. That was not possible. 
There are 178.29M free.; (0x0x0_HBM0)
```

### Progress Analysis

**Loading Progress Comparison**:
- **First attempt**: Failed at 12% of weights loaded
- **32MB threshold**: Failed much later, completed many layers
- **Memory efficiency**: 32MB threshold distributing more tensors across MODEL axis

**Problem**: Still hitting memory limit in `jax.make_array_from_callback` during loading

### Next Solution: Even More Aggressive Sharding

**Option**: Lower threshold further: `32MB` ‚Üí `16MB` or `8MB`
**Rationale**: We're very close (178MB free vs 208MB needed = only 30MB short)
**Target**: Get even more tensors using MODEL axis (16 devices) instead of DATA axis (8 devices)

**Alternative**: Add memory cleanup/garbage collection between loading phases

## üöÄ NEXT ATTEMPT: 16MB Threshold - Final Push

**Implementation Plan**:
1. **Lower threshold**: `tensor_size_mb > 32` ‚Üí `tensor_size_mb > 16`
2. **Target smaller tensors**: Force more tensors to use MODEL axis (16 devices)
3. **Expected gain**: Only need 30MB more free memory (208MB - 178MB = 30MB gap)

**Tensors that will switch from DATA to MODEL axis**:
- Any tensors 16MB-32MB range will now use MODEL axis instead of DATA axis
- 16MB tensor: DATA axis = 2MB/device ‚Üí MODEL axis = 1MB/device (50% reduction)
- Multiple 16-32MB tensors √ó 50% reduction = should free up the needed 30MB+

**Risk**: Very small tensors might not need MODEL axis distribution
**Mitigation**: Still have DATA axis fallback for tiny tensors (<16MB)

**‚úÖ Implementation Applied**:
1. **Modified threshold**: `tensor_size_mb > 32` ‚Üí `tensor_size_mb > 16`
2. **Updated config name**: `llama_3.1_405b_hp1_token_v4_256_model16_tp_aggressive_16mb`

**Expected Debug Output**:
- 16MB+ tensors: `"Large tensor detected, trying MODEL axis"`
- More tensors distributed across 16 devices instead of 8 devices
- Should free up the critical 30MB gap needed for loading success

---

## üîç CRITICAL INSIGHT: Need Better Progress Visibility

### User's Critical Insight ‚≠ê
The progress bars like `Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2` are **per-shard progress**, not overall model loading progress. **We cannot actually tell if we're making progress through the overall model or hitting the same layers repeatedly.**

This completely changes our understanding of whether the 32MB threshold "improvements" were real progress or just completing the same early layers in different shard files.

### Added Comprehensive Logging ‚úÖ

**New Debug Output** in `src/levanter/compat/hf_checkpoints.py`:

1. **Overall Progress**:
   ```
   [DEBUG] Total shards to load: X
   [DEBUG] Loading shard Y/X: shard-file-name.safetensors
   [DEBUG] Shard Y/X loaded successfully, Z parameters
   ```

2. **Parameter-Level Progress**:
   ```
   [DEBUG] Loading W parameters from shard: /path/to/shard
   [DEBUG] Loading parameter N/W: model.layers.0.self_attn.q_proj.weight
   [DEBUG] Parameter shape: [16384, 16384]
   [DEBUG] Parameter loaded successfully / FAILED loading parameter: error
   ```

### What This Will Reveal:
- **Exact parameter that fails**: Which layer/component causes OOM
- **True progress**: Are we advancing through layers (0‚Üí1‚Üí2) or stuck on layer 0?
- **Memory usage pattern**: Which parameter sizes/shapes trigger OOM
- **Overall completion**: How far through the 405B model we actually get

**Updated config**: `llama_3.1_405b_hp1_token_v4_256_model16_tp_debug_logging`

This will give us definitive answers about whether our sharding optimizations are actually helping with overall model loading progress.

---

## ‚ö†Ô∏è CRITICAL ANALYSIS: Both 16MB and 32MB Thresholds Fail Similarly

### Key Findings from 16mb_progress.log vs 32mb_progress.log

**1. MAJOR HARDWARE UNDERUTILIZATION ‚ùå**
- **v4-256 has 128 devices total** (32 workers √ó 4 devices each)
- **We're only using 16 devices for model axis** ‚Üí **wasting 87.5% of available devices!**
- **Mesh shape**: `(1, 8, 16)` = 128 devices ‚úÖ but poor distribution

**2. LOADING PATTERN IS NON-SEQUENTIAL ‚ö†Ô∏è**
- **16MB log**: Layer 20-21 ‚Üí Layer 100 ‚Üí ... ‚Üí **fails at Layer 98**
- **32MB log**: Similar random order ‚Üí **fails at Layer 76**  
- **191 total shards** - layers scattered across shards, not sequential loading

**3. BOTH FAIL AT ~8 MINUTES WITH SIMILAR MEMORY PATTERNS**
- **16MB**: `208MB needed, 199.27MB free` (only 8.73MB short!)
- **32MB**: `208MB needed, 127.27MB free` (80MB short)
- **Same 208MB allocation request** - suggests same operation failing

**4. SHARDING IS WORKING BUT INSUFFICIENT**
- ‚úÖ Large tensors using MODEL axis: `PartitionSpec(None, 'model')`
- ‚úÖ 3328MB tensors distributed across 16 devices ‚Üí 208MB per device
- ‚ùå **But 208MB per device still exceeds available memory**

### ROOT CAUSE: DEVICE MESH CONFIGURATION WRONG

**Current**: `model_axis_size: 16` uses only 16/128 devices for tensor parallelism
**Should be**: `model_axis_size: 64` or `model_axis_size: 32` for proper utilization

**Memory calculation**:
- 3328MB tensor √∑ 16 devices = 208MB per device ‚ùå (too much)
- 3328MB tensor √∑ 64 devices = 52MB per device ‚úÖ (manageable)
- 3328MB tensor √∑ 32 devices = 104MB per device ‚úÖ (borderline but better)

## üöÄ NEXT SOLUTION: Fix Device Mesh Configuration

**Change needed**: `model_axis_size: 16` ‚Üí `model_axis_size: 64`

**New mesh shape**: `(1, 2, 64)` instead of `(1, 8, 16)`
- **Model axis**: 64 devices (4x increase)
- **Data axis**: 2 devices (batch parallelism)
- **Total**: 128 devices ‚úÖ (full utilization)

**Expected improvement**:
- 3328MB tensors: 208MB/device ‚Üí 52MB/device (75% reduction)
- Should easily fit within available memory per device
- Better utilization of v4-256 hardware

**Risk**: Need to verify 2 devices is sufficient for data/batch axis
**Alternative**: `model_axis_size: 32` ‚Üí mesh `(1, 4, 32)` for safer batch parallelism

---

## üéâ MAJOR SUCCESS + NEW ISSUE: Full Parallelism Analysis

### Analysis of full_parallel.log

**‚úÖ MASSIVE PROGRESS ACHIEVED:**
1. **All 191/191 shards loaded successfully!** üéâ
2. **Runtime**: 1308.4 seconds = **~22 minutes** (vs 8 minutes failure before)
3. **Perfect sharding**: 3328MB tensors ‚Üí 26MB per device across 128 devices
4. **Final layer loaded**: Layer 116 (much further than Layer 76/98 before)
5. **Mesh working perfectly**: `(1, 1, 128)` using all devices

**‚ùå NEW FAILURE AFTER MODEL LOADING:**
```
[DEBUG] [1308.4s] Shard 191/191 loaded successfully, 2 parameters, took 5.4s
jaxlib._jax.XlaRuntimeError: RESOURCE_EXHAUSTED: 
Attempting to allocate 126.00G. That was not possible. 
There are 18.92G free.; (0x0x0_HBM0): 
while running replica 0 and partition 0 of a replicated computation
```

### Root Cause Analysis

**Location**: `haliax/partitioning.py:354` in `load_from_state_dict`
**Problem**: After successful model loading, the model conversion/initialization step fails
**Issue**: 126GB allocation request suggests model reconstruction/conversion phase

**Key insight**: "replica 0 and partition 0" suggests the issue is in model assembly, not loading
- Individual parameters loaded fine with sharding
- Failure happens when assembling/initializing the complete model
- 126GB = ~1/4 of full model, suggests partial model reconstruction

### Next Steps

**The tensor parallelism worked perfectly for loading!** 
**New bottleneck**: Model assembly/conversion after loading completes

**Potential solutions**:
1. Check if `load_from_state_dict` respects axis_mapping
2. Investigate model conversion memory requirements  
3. May need different approach for model initialization vs loading

---

## üìã DETAILED SESSION SUMMARY - Current Status & Next Steps

### ‚úÖ MAJOR BREAKTHROUGHS ACHIEVED

**1. Root Cause Diagnosis Complete:**
- **Initial Problem**: `best_effort_sharding` function ignored MODEL axis, only used DATA axis
- **Hardware Issue**: Using only 16/128 devices on v4-256, wasting 87.5% of resources
- **Memory Issue**: 3328MB tensors ‚Üí 208MB per device (too much) vs 26MB per device (manageable)

**2. Successful Model Loading:**
- **Full tensor parallelism**: `model_axis_size: 128` using ALL devices
- **All 191/191 shards loaded successfully** (vs previous ~76-98 shard failures)
- **22 minutes runtime** - reached much deeper into model (Layer 116+)
- **Perfect sharding**: Large tensors use `PartitionSpec(None, 'model')` across 128 devices

### ‚ùå CURRENT BOTTLENECK: Model Assembly/Partitioning

**New Failure Location**: `haliax/partitioning.py:354` in `load_from_state_dict`
- **Issue**: 126GB allocation request after successful loading
- **Error**: "while running replica 0 and partition 0 of a replicated computation"
- **Problem**: Model conversion/assembly phase not respecting tensor parallelism

### üîß DEBUGGING SETUP APPLIED

**Added comprehensive logging in `haliax/src/haliax/partitioning.py`:**
- `WrappedCallable._call` method debugging
- Axis resource mapping tracking
- Sharding specification logging
- Execution point identification (where 126GB allocation fails)

**Config**: `llama_3.1_405b_hp1_token_v4_256_model128_debug_partitioning`

### üìä KEY TECHNICAL DETAILS

**Hardware Configuration:**
- **v4-256**: 128 devices total (32 workers √ó 4 devices each)
- **Mesh**: `(1, 1, 128)` ‚Üí `('replica', 'data', 'model')`
- **Perfect utilization**: All 128 devices used for model parallelism

**Memory Distribution:**
- **3328MB tensors**: 26MB per device (well within limits)
- **Loading successful**: All parameters fit with proper sharding
- **Assembly failure**: 126GB allocation suggests model reconstruction issue

### üéØ NEXT SESSION PRIORITIES

**1. Analyze Debug Logs:**
- Run with `debug_partitioning` config
- Identify exact point where 126GB allocation happens
- Check axis_mapping context during model assembly

**2. Investigate `load_from_state_dict`:**
- Verify if model assembly respects `axis_mapping=pmapping`
- Check if model conversion phase ignores tensor parallelism
- May need to modify model initialization approach

**3. Potential Solutions:**
- Force model assembly to use same sharding as loading
- Investigate alternative model loading/conversion approaches
- Check if different initialization order helps

### üìù FILES MODIFIED FOR DEBUGGING

1. **`src/levanter/compat/hf_checkpoints.py`**: Added shard/parameter loading progress
2. **`src/levanter/utils/jax_utils.py`**: Fixed `best_effort_sharding` for tensor parallelism
3. **`haliax/src/haliax/partitioning.py`**: Added detailed partitioning debug logs
4. **`config/books/eval_careless_llama3.1_405b_hp1.yaml`**: Full model parallelism config

**Status**: üü¢ Model loading completely solved, üî¥ Model assembly/partitioning remains

## Configuration References
- **Axis Mapping Context**: `hax.axis_mapping()` for scoped partitioning
- **Sharding Function**: `hax.shard()` for array distribution  
- **JIT Integration**: `named_jit()` with axis resources for distributed compilation